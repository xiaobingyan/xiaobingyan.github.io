<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>pyspark学习笔记 | Zoe.Xiao&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="pyspark" />
  
  
  
  
  <meta name="description" content="目前：ubuntu 14.04，spark2.2.0-bin-hadoop2.7，pycharm，jupyter notebook，IPython。 主要参考以下来源自己做的梳理。PySpark API DocSpark 2.2.0 RDD中文手册">
<meta name="keywords" content="pyspark">
<meta property="og:type" content="article">
<meta property="og:title" content="pyspark学习笔记">
<meta property="og:url" content="https://xiaobingyan.github.io/2018/03/03/pyspark_note/index.html">
<meta property="og:site_name" content="Zoe.Xiao&#39;s Blog">
<meta property="og:description" content="目前：ubuntu 14.04，spark2.2.0-bin-hadoop2.7，pycharm，jupyter notebook，IPython。 主要参考以下来源自己做的梳理。PySpark API DocSpark 2.2.0 RDD中文手册">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2018-03-03T07:39:47.752Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="pyspark学习笔记">
<meta name="twitter:description" content="目前：ubuntu 14.04，spark2.2.0-bin-hadoop2.7，pycharm，jupyter notebook，IPython。 主要参考以下来源自己做的梳理。PySpark API DocSpark 2.2.0 RDD中文手册">
  
    <link rel="alternate" href="/atom.xml" title="Zoe.Xiao&#39;s Blog" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">

  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Oswald%3A300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >
  <link rel="stylesheet" href="/css/fashion.css" >
  <link rel="stylesheet" href="/css/glyphs.css" >

</head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  
  <div class="site-header-image">
    <img id="originBg" width="100%" alt="Hike News" src="">
  </div>

  <div id="header-blur" class="site-header-image blur" style="position: absolute; top:0; height: 207px; min-height: 207px; min-width: 100%;">
    <img id="blurBg" width="100%" style="top: 96%" alt="Hike News" src="">
  </div>

  <script>
        var imgUrls = "css/images/pose01.jpg".split(",");
        var random = Math.floor((Math.random() * imgUrls.length ));
        if (imgUrls[random].startsWith('http') || imgUrls[random].indexOf('://') >= 0) {
          document.getElementById("originBg").src=imgUrls[random];
          document.getElementById("blurBg").src=imgUrls[random];
        } else {
          document.getElementById("originBg").src='/' + imgUrls[random];
          document.getElementById("blurBg").src='/' + imgUrls[random];
        }
    </script>




<header id="allheader" class="site-header" role="banner" 
   style="width: 100%; position: absolute; top:0; background: rgba(255,255,255,.8);"  >
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="Zoe.Xiao&#39;s Blog" rel="home"> Zoe.Xiao&#39;s Blog </a>
            
          </h1>
          
          
            <div class="site-description">缓解焦虑</div>
          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>

            <div class="clearfix sf-menu">
              <ul id="main-nav" class="menu sf-js-enabled sf-arrows"  style="touch-action: pan-y;">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">首页</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">归档</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories">分类</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/tags">标签</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about">关于</a> </li>
                    
              </ul>
            </div>
          </nav>

      </div>
  </div>
</header>


  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-pyspark_note" style="width: 66%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      pyspark学习笔记
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2018/03/03/pyspark_note/" class="article-date">
	  <time datetime="2018-03-03T07:20:05.000Z" itemprop="datePublished">三月 3, 2018</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/spark/">spark</a>

      
	<span id="busuanzi_container_page_pv">
	  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
	</span>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>目前：ubuntu 14.04，spark2.2.0-bin-hadoop2.7，pycharm，jupyter notebook，IPython。</p>
<p>主要参考以下来源自己做的梳理。<br><a href="http://spark.apache.org/docs/2.2.0/api/python/index.html" target="_blank" rel="noopener">PySpark API Doc</a><br><a href="http://spark.apachecn.org/docs/cn/2.2.0/rdd-programming-guide.html" target="_blank" rel="noopener">Spark 2.2.0 RDD中文手册</a></p>
<a id="more"></a>
<p>要想在Python shell中使用Spark，直接运行./bin/pyspark命令即可，如果配置了pyspark的环境变量，则直接运行pyspark命令即可。与Scala shell类似， Python下的SparkContext对象可以通过Python变量sc来调用</p>
<h3 id="主要结构介绍"><a href="#主要结构介绍" class="headerlink" title="主要结构介绍"></a>主要结构介绍</h3><p>Spark目前主要提供了以下6大功能。</p>
<ol>
<li>Spark Core: RDD及其算子。</li>
<li>Spark-SQL: DataFrame与SQL。</li>
<li>Spark ML(MLlib): 机器学习框架。</li>
<li>Spark Streaming: 实时计算框架。</li>
<li>Spark GraphX: 图计算框架。</li>
<li>PySpark(SparkR): Spark之上的Python与R框架。</li>
</ol>
<h4 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h4><p>resilient distributed dataset 完全弹性分布式的，因为有replicate机制所以部分数据丢失可以重建，高容错，在集群当中若几台节点故障一样可以正常运行，高scalability，横向扩展直接多增加集群节点可以直接提高性能.。Spark主要通过sparkcontext.textfile 加载文件变成RDD，然后通过transformation构建一个新的RDD，通过action将RDD存储到外部节点.</p>
<p>RDD对象是不可变的。一旦对象被创建，它们的值就无法再变化。</p>
<p>RDD有一头delay 加载，换句话说类似Dask的 lazy evaluation，只有用得到的时候才加载数据，如果加载存储所有的中间过程会浪费空间，这样会影响spark的性能. 一旦spark看到整个变换链，他可以计算仅需要结果数据，如果下面的函数不需要数据那么也不会再加载，转换RDD也是lazy的只有actions的时候才会调用它们。</p>
<p>Spark分为driver和executor，driver提交作业，executor是application早worknode上的进程，运行task，driver对应为sparkcontext。</p>
<p>RDD有两种操作算子：</p>
<ul>
<li>Transformation（转换）：Transformation属于延迟计算，当一个RDD转换成另一个RDD时并没有立即进行转换，仅仅是记住了数据集的逻辑操作（如<code>map()</code>、<code>reduceByKey()</code>）。总是返回对一个RDD对象的引用。</li>
<li>Ation（执行）：触发Spark作业的运行，真正触发转换算子的计算（<code>take()</code>，<code>reduce()</code>，<code>saveAsTextFile()</code>，<code>collect()</code>）。</li>
</ul>
<p>Spark的RDD操作有transformation、action。Transformation对RDD进行依赖包装，RDD所对应的依赖都进行DAG的构建并保存，在worknode挂掉之后除了通过备份恢复还可以通过元数据对其保存的依赖再计算一次得到。当作业提交也就是调用runJob时，spark会根据RDD构建DAG图，提交给DAGScheduler，这个DAGScheduler是在SparkContext创建时一同初始化的，他会对作业进行调度处理。当依赖图构建好以后，从action开始进行解析，每一个操作作为一个task，每遇到shuffle就切割成为一个taskSet，并把数据输出到磁盘，如果不是shuffle数据还在内存中存储。就这样再往前推进，直到没有算子，然后运行从前面开始，如果没有action的算子在这里不会执行，直到遇到action为止才开始运行，这就形成了spark的懒加载，taskset提交给TaskSheduler生成TaskSetManager并且提交给Executor运行，运行结束后反馈给DAGScheduler完成一个taskSet，之后再提交下一个，当TaskSet运行失败时就返回DAGScheduler并重新再次创建。一个job里面可能有多个TaskSet，一个application可能包含多个job。</p>
<h4 id="Spark-streaming"><a href="#Spark-streaming" class="headerlink" title="Spark streaming"></a>Spark streaming</h4><p>通过对kafka数据读取，将Stream数据分成小的时间片段（几秒），以类似batch批处理的方式来处理这一部分小数据，每个时间片生成一个RDD，有高效的容错性，对小批量数据可以兼容批量实时数据处理的逻辑算法，用一些历史数据和实时数据联合进行分析，比如分类算法等。也可以对小批量的stream进行mapreduce、join等操作，而保证其实时性。针对数据流时间要求不到毫秒级的工程性问题都可以。</p>
<p>Spark Streaming也有一个StreamingContext，其核心是DStream，是通过以组时间序列上的连续RDD来组成的，包含一个有Time作为key、RDD作为value的结构体，每一个RDD都包含特定时间间隔的数据流，可以通过persist将其持久化。在接受不断的数据流后，在blockGenerator中维护一个队列，将流数据放到队列中，等处理时间间隔到来后将其中的所有数据合并成为一个RDD(这一间隔中的数据)。其作业提交和spark相似，只不过在提交时拿到DStream内部的RDD并产生Job提交，RDD在action触发之后，将job提交给jobManager中的JobQueue，又jobScheduler调度，JobScheduler将job提交到spark的job调度器，然后将job转换成为大量的任务分发给spark集群执行。Job从outputStream中生成的，然后触发反向回溯执行DStreamDAG。在流数据处理的过程中，一般节点失效的处理比离线数据要复杂。Spark streamin在1.3之后可以周期性的将DStream写入HDFS，同时将offset也进行存储，避免写到zk。一旦主节点失效，会通过checkpoint的方式读取之前的数据。当worknode节点失效，如果HDFS或文件作为输入源那Spark会根据依赖关系重新计算数据，如果是基于Kafka、Flume等网络数据源spark会将手机的数据源在集群中的不同节点进行备份，一旦有一个工作节点失效，系统能够根据另一份还存在的数据重新计算，但是如果接受节点失效会丢失一部分数据，同时接受线程会在其他的节点上重新启动并接受数据。</p>
<h4 id="Graphx"><a href="#Graphx" class="headerlink" title="Graphx"></a>Graphx</h4><p>主要用于图的计算。核心算法有PageRank、SVD奇异矩阵、TriangleConut等。</p>
<h4 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h4><p>是Spark新推出的交互式大数据SQL技术。把sql语句翻译成Spark上的RDD操作可以支持Hive、Json等类型的数据。</p>
<h3 id="SparkContext类和SparkConf类"><a href="#SparkContext类和SparkConf类" class="headerlink" title="SparkContext类和SparkConf类"></a>SparkContext类和SparkConf类</h3><p>具体pyspark.SparkContext包含的<a href="http://spark.apache.org/docs/2.2.0/api/python/pyspark.html#pyspark.SparkContext" target="_blank" rel="noopener">方法</a></p>
<p>SparkContext 是管理Spark里的集群（cluster）和协调集群运行进程的对象，功能的主入口。SparkContext与集群的manager相连。Manager负责管理运行具体运算的执行者。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class pyspark.SparkContext(master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=&lt;class &apos;pyspark.profiler.BasicProfiler&apos;&gt;)</span><br></pre></td></tr></table></figure></p>
<p>SparkConf用于初始化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conf = SparkConf().setAppName(appName).setMaster(master)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br></pre></td></tr></table></figure>
<p><code>appName</code> 参数是一个在集群 UI 上展示应用程序的名称。 <code>master</code> 是一个 Spark, Mesos 或 YARN 的 cluster URL，或者指定为在 local mode（本地模式）中运行的 “local” 字符串。</p>
<p>Spark程序的编写都是从SparkContext（或用Java编写时的JavaSparkContext）开始的。SparkContext的初始化需要一个<code>SparkConf</code>对象，后者包含了Spark集群配置的各种参数（比如主节点的URL），用于将各种Spark参数设置为键值对。<br>。初始化后，我们便可用<code>SparkContext</code>对象所包含的各种方法来创建和操作分布式数据集和共享变量。Spark shell（在Scala和Python下可以，但不支持Java）能自动完成上述初始化。<br>我们可以通过如下方式调SparkContext的简单构造函数，以默认的参数值来创建4线程的SparkContext对象，并将其相应的任务命名为Test SparkAPP。：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val sc = new SparkContext(&quot;local[4]&quot;, &quot;Test Spark App&quot;)</span><br></pre></td></tr></table></figure></p>
<h4 id="广播变量（broadcast）和累加器（accumulator）"><a href="#广播变量（broadcast）和累加器（accumulator）" class="headerlink" title="广播变量（broadcast）和累加器（accumulator）"></a>广播变量（broadcast）和累加器（accumulator）</h4><p>通常情况下，一个传递给 Spark 操作（例如 map 或 reduce）的函数 func 是在远程的集群节点上执行的。该函数 func 在多个节点执行过程中使用的变量，是同一个变量的多个副本。这些变量的以副本的方式拷贝到每个机器上，并且各个远程机器上变量的更新并不会传播回 driver program（驱动程序）。通用且支持 read-write（读-写） 的共享变量在任务间是不能胜任的。所以，Spark 提供了两种特定类型的共享变量 : broadcast variables（广播变量）和 accumulators（累加器）。</p>
<p><strong>广播变量</strong>（broadcast）为<strong>只读</strong>变量，由运行SparkContext的驱动程序创建后发送给会参与计算的节点，可以被工作节点访问。对那些需要各个工作节点高效访问相同数据的应用场景十分有用。</p>
<p>Spark 的 action（动作）操作是通过一系列的 stage（阶段）进行执行的，这些 stage（阶段）是通过分布式的 “shuffle” 操作进行拆分的。Spark 会自动广播出每个 stage（阶段）内任务所需要的公共数据。这种情况下广播的数据使用序列化的形式进行缓存，并在每个任务运行前进行反序列化。这也就意味着，只有在跨越多个 stage（阶段）的多个任务会使用相同的数据，或者在使用反序列化形式的数据特别重要的情况下，使用广播变量会有比较好的效果。</p>
<p>广播变量通过在一个变量<code>v</code> 上调用 <code>SparkContext.broadcast(v)</code> 方法来进行创建。广播变量是 <code>v</code> 的一个 wrapper（包装器），可以通过调用 <code>value</code> 方法来访问它的值。代码示例如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; broadcastVar = sc.broadcast([1, 2, 3])</span><br><span class="line">&lt;pyspark.broadcast.Broadcast object at 0x102789f10&gt;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; broadcastVar.value</span><br><span class="line">[1, 2, 3]</span><br></pre></td></tr></table></figure>
<p>在创建广播变量之后，在集群上执行的所有的函数中，应该使用该广播变量代替原来的 v 值，所以节点上的 v 最多分发一次。另外，对象 v 在广播后不应该再被修改，以保证分发到所有的节点上的广播变量具有同样的值（例如，如果以后该变量会被运到一个新的节点）。</p>
<p><strong>累加器</strong>（accumulator）也是一种被广播到工作节点的变量，是一个仅可以执行 “added”（添加）的变量，但是支持的累加操作有一定的限制。这种累加得保证在全局范围内累加起来的值能被正确地并行计算以及返回驱动程序。每一个工作节点只能访问和操作自己本地的累加器，全局累加器则只允许驱动程序访问。</p>
<p>累加器可以用于实现 counter（ 计数，类似在 MapReduce 中那样）或者 sums（求和）。原生 Spark 支持数值型的累加器，并且程序员可以添加新的支持类型。</p>
<p>作为一个用户，您可以创建 accumulators（累加器）并且重命名。, 一个命名的 accumulator 累加器（在这个例子中是 counter）将显示在 web UI 中，用于修改该累加器的阶段。 Spark 在 “Tasks” 任务表中显示由任务修改的每个累加器的值.</p>
<p>python下通过<code>sc</code>调用SparkContext。</p>
<p>通过调用·SparkContext.accumulator(v)<code>从初始值v创建一个累加器。 然后可以使用</code>add<code>方法或</code>+=`运算符将在群集上运行的任务添加到其中。 但是，他们看不到它的值。 只有驱动程序可以使用value方法读取累加器的值。</p>
<p>下面的代码显示了一个累加器用于将数组的元素相加：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; accum = sc.accumulator(0)</span><br><span class="line">&gt;&gt;&gt; accum</span><br><span class="line">Accumulator&lt;id=0, value=0&gt;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))</span><br><span class="line">...</span><br><span class="line">10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; accum.value</span><br><span class="line">10</span><br></pre></td></tr></table></figure></p>
<p>虽然此代码使用Int类型的累加器的内置支持，但程序员也可以通过对<code>AccumulatorParam</code>进行子类化来创建自己的类型。 <code>AccumulatorParam</code>接口有两种方法：<code>zero</code>为您的数据类型提供“零值”，<code>addInPlace</code>用于将两个值相加在一起。 例如，假设我们有一个代表数学向量的<code>Vector</code>类，我们可以写：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class VectorAccumulatorParam(AccumulatorParam):</span><br><span class="line">    def zero(self, initialValue):</span><br><span class="line">        return Vector.zeros(initialValue.size)</span><br><span class="line"></span><br><span class="line">    def addInPlace(self, v1, v2):</span><br><span class="line">        v1 += v2</span><br><span class="line">        return v1</span><br><span class="line"></span><br><span class="line"># Then, create an Accumulator of this type:</span><br><span class="line">vecAccum = sc.accumulator(Vector(...), VectorAccumulatorParam())</span><br></pre></td></tr></table></figure></p>
<p>累加器的更新只发生在 action 操作中，Spark 保证每个任务只更新累加器一次，例如，重启任务不会更新值。在 transformations（转换）中， 用户需要注意的是，如果 task（任务）或 job stages（阶段）重新执行，每个任务的更新操作可能会执行多次。</p>
<p>累加器不会改变 Spark lazy evaluation（懒加载）的模式。如果累加器在 RDD 中的一个操作中进行更新，它们的值仅被更新一次，RDD 被作为 action 的一部分来计算。因此，在一个像 map() 这样的 transformation（转换）时，累加器的更新并没有执行。下面的代码片段证明了这个特性:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">accum = sc.accumulator(0)</span><br><span class="line">def g(x):</span><br><span class="line">    accum.add(x)</span><br><span class="line">    return f(x)</span><br><span class="line">data.map(g)</span><br><span class="line"># Here, accum is still 0 because no actions have caused the `map` to be computed.</span><br></pre></td></tr></table></figure></p>
<h4 id="parallelize和分区（partitions）"><a href="#parallelize和分区（partitions）" class="headerlink" title="parallelize和分区（partitions）"></a>parallelize和分区（partitions）</h4><p><code>parallelize(c, numSlices = None)</code>通过在驱动程序中现有的迭代或集合上调用SparkContext的<code>parallelize</code>方法来创建分发本地Python集合以形成RDD。if the input represents a range for performance，则建议使用xrange。 集合的元素被复制以形成可以并行操作的分布式数据集。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()</span><br><span class="line">[[0], [2], [3], [4], [6]]</span><br><span class="line">&gt;&gt;&gt; sc.parallelize(xrange(0, 6, 2), 5).glom().collect()</span><br><span class="line">[[], [0], [], [2], [4]]</span><br></pre></td></tr></table></figure></p>
<p>并行集合中一个很重要参数是 partitions（分区）的数量，它可用来切割 dataset（数据集）。Spark 将在集群中的每一个分区上运行一个任务。通常您希望群集中的每一个 CPU 计算 2-4 个分区。一般情况下，Spark 会尝试根据您的群集情况来自动的设置的分区的数量。当然，您也可以将分区数作为第二个参数传递到 parallelize方法中 (e.g. <code>sc.parallelize(data, 10)</code>)来手动的设置它。注意: 代码中的一些地方会使用 term slices (a synonym for partitions) 以保持向后兼容。</p>
<p>可以使用<code>data.partitions.size</code>来查询一个RDD被划分的分区数。</p>
<h4 id="textFile"><a href="#textFile" class="headerlink" title="textFile"></a>textFile</h4><p><code>textFile(name, minPartitions=None, use_unicode=True</code><br>从HDFS，本地文件系统（在所有节点上可用）或任何Hadoop支持的文件系统URI读取文本文件，并将其作为字符串的RDD返回。</p>
<p>如果use_unicode为False，则字符串将保持为str（编码为utf-8），它比unicode更快更小。 （在Spark 1.2中添加）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; path = os.path.join(tempdir, &quot;sample-text.txt&quot;)</span><br><span class="line">&gt;&gt;&gt; with open(path, &quot;w&quot;) as testFile:</span><br><span class="line">...    _ = testFile.write(&quot;Hello world!&quot;)</span><br><span class="line">&gt;&gt;&gt; textFile = sc.textFile(path)</span><br><span class="line">&gt;&gt;&gt; textFile.collect()</span><br><span class="line">[u&apos;Hello world!&apos;]</span><br></pre></td></tr></table></figure></p>
<h4 id="wholeTextFiles-path-minPartitions-None-sue-unicode-True"><a href="#wholeTextFiles-path-minPartitions-None-sue-unicode-True" class="headerlink" title="wholeTextFiles(path,minPartitions = None,sue_unicode = True)"></a>wholeTextFiles(path,minPartitions = None,sue_unicode = True)</h4><p>从HDFS，本地文件系统（在所有节点上可用）或任何Hadoop支持的文件系统URI读取文本文件的目录。 每个文件被读取为单个记录并返回到键值对中，其中键是每个文件的路径，该值是每个文件的内容。</p>
<p>例如有以下文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hdfs://a-hdfs-path/part-00000</span><br><span class="line">hdfs://a-hdfs-path/part-00001</span><br><span class="line">...</span><br><span class="line">hdfs://a-hdfs-path/part-nnnnn</span><br></pre></td></tr></table></figure></p>
<p>使用语句<code>rdd = sparkContext.wholeTextFiles(&#39;hdfs://a-hdfs-path&#39;)</code><br>rdd中包含<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(a-hdfs-path/part-00000, its content)</span><br><span class="line">(a-hdfs-path/part-00001, its content)</span><br><span class="line">...</span><br><span class="line">(a-hdfs-path/part-nnnnn, its content)</span><br></pre></td></tr></table></figure></p>
<h4 id="addFile-path-recursive-False"><a href="#addFile-path-recursive-False" class="headerlink" title="addFile(path,recursive=False)"></a>addFile(path,recursive=False)</h4><p>在每个节点上添加使用此Spark作业要下载的文件。 传递的路径可以是本地文件，HDFS（或其他Hadoop支持的文件系统）中的文件，或HTTP，HTTPS或FTP URI。</p>
<p>要在Spark作业中访问文件，请使用文件名L {SparkFiles.get（fileName）&lt;pyspark.files.SparkFiles.get&gt;}来查找其下载位置。<br><em>class pyspark.SparkFiles</em>：<br>SparkFiles只包含类方法; 用户不应该创建SparkFiles实例。有以下两个方法<br>classmethod <code>get(filename)</code><br>获取通过<code>SparkContext.addFile()</code>添加的文件的绝对路径。</p>
<p>classmethod getRootDirectory()<br>获取包含通过<code>SparkContext.addFile()</code>添加的文件的根目录。</p>
<p>如果递归选项设置为True，则可以给出目录。 目前仅支持Hadoop支持的文件系统。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from pyspark import SparkFiles</span><br><span class="line">&gt;&gt;&gt; path = os.path.join(tempdir, &quot;test.txt&quot;)</span><br><span class="line">&gt;&gt;&gt; with open(path, &quot;w&quot;) as testFile:</span><br><span class="line">...    _ = testFile.write(&quot;100&quot;)</span><br><span class="line">&gt;&gt;&gt; sc.addFile(path)</span><br><span class="line">&gt;&gt;&gt; def func(iterator):</span><br><span class="line">...    with open(SparkFiles.get(&quot;test.txt&quot;)) as testFile:</span><br><span class="line">...        fileVal = int(testFile.readline())</span><br><span class="line">...        return [x * fileVal for x in iterator]</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()</span><br><span class="line">[100, 200, 300, 400]</span><br></pre></td></tr></table></figure></p>
<h4 id="addPyFile-path"><a href="#addPyFile-path" class="headerlink" title="addPyFile(path)"></a>addPyFile(path)</h4><p>为将来在此SparkContext上执行的所有任务添加.py或.zip依赖项。 传递的路径可以是本地文件，HDFS（或其他Hadoop支持的文件系统）中的文件，或HTTP，HTTPS或FTP URI。</p>
<h4 id="pickleFile"><a href="#pickleFile" class="headerlink" title="pickleFile"></a>pickleFile</h4><p>加载先前使用RDD.saveAsPickleFile方法保存的RDD。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; tmpFile = NamedTemporaryFile(delete=True)</span><br><span class="line">&gt;&gt;&gt; tmpFile.close()</span><br><span class="line">&gt;&gt;&gt; sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)</span><br><span class="line">&gt;&gt;&gt; sorted(sc.pickleFile(tmpFile.name, 3).collect())</span><br><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p>
<h4 id="sequenceFile"><a href="#sequenceFile" class="headerlink" title="sequenceFile"></a>sequenceFile</h4><p><code>sequenceFile(path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)</code><br>从HDFS，本地文件系统（在所有节点上可用）或任何Hadoop支持的文件系统URI读取具有任意键和值的Hadoop SequenceFile可写类。 机制如下：</p>
<h4 id="cancelAllJobs"><a href="#cancelAllJobs" class="headerlink" title="cancelAllJobs()"></a>cancelAllJobs()</h4><p>取消已安排或正在运行的所有作业。</p>
<h4 id="cancelJobGroup-groupId"><a href="#cancelJobGroup-groupId" class="headerlink" title="cancelJobGroup(groupId)"></a>cancelJobGroup(groupId)</h4><p>取消指定组的活动作业。 有关更多信息，请参阅<a href="http://spark.apache.org/docs/2.2.0/api/python/pyspark.html#pyspark.SparkContext.setJobGroup" target="_blank" rel="noopener">SparkContext.setJobGroup</a>。</p>
<h4 id="setJobGroup"><a href="#setJobGroup" class="headerlink" title="setJobGroup"></a>setJobGroup</h4><p>将组ID分配给此线程启动的所有作业，直到组ID设置为不同的值或清除为止。</p>
<p>通常，应用程序中的执行单元由多个Spark操作或作业组成。 应用程序员可以使用此方法将所有这些作业分组在一起并给出组描述。 一旦设置，Spark Web UI将将这些作业与该组关联。</p>
<p>该应用程序可以使用<code>SparkContext.cancelJobGroup</code>取消此组中的所有正在运行的作业。</p>
<h4 id="runJob-rdd-partitionFunc-partitions-None-allowLocal-False"><a href="#runJob-rdd-partitionFunc-partitions-None-allowLocal-False" class="headerlink" title="runJob(rdd,partitionFunc, partitions=None, allowLocal=False)"></a>runJob(rdd,partitionFunc, partitions=None, allowLocal=False)</h4><p>在指定的分区集上执行给定的partitionFunc，将结果作为元素数组返回。</p>
<p>如果未指定“分区”，则将在所有分区上运行。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; myRDD = sc.parallelize(range(6), 3)</span><br><span class="line">&gt;&gt;&gt; sc.runJob(myRDD, lambda part: [x * x for x in part])</span><br><span class="line">[0, 1, 4, 9, 16, 25]</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; myRDD = sc.parallelize(range(6), 3)</span><br><span class="line">&gt;&gt;&gt; sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)</span><br><span class="line">[0, 1, 16, 25]//[0, 1], [2, 3], [4, 5]</span><br></pre></td></tr></table></figure>
<h4 id="emptyRDD"><a href="#emptyRDD" class="headerlink" title="emptyRDD()"></a>emptyRDD()</h4><p>创建一个没有分区或元素的RDD。</p>
<h4 id="hadoopRdd"><a href="#hadoopRdd" class="headerlink" title="hadoopRdd"></a>hadoopRdd</h4><p><code>hadoopRDD(inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)</code><br>从任意的Hadoop配置中读取具有任意键和值类的“旧”Hadoop InputFormat，该配置以Python语法传入。 这将被转换为Java中的配置。 机制与sc.sequenceFile相同。</p>
<h4 id="getConf"><a href="#getConf" class="headerlink" title="getConf()"></a>getConf()</h4><h4 id="getLocalProperty-key"><a href="#getLocalProperty-key" class="headerlink" title="getLocalProperty(key)"></a>getLocalProperty(key)</h4><p>获取此线程中的本地属性集，如果缺少则为null。 请参阅<code>setLocalProperty</code><br><em>classmethod</em> <code>getOrCreate(conf=None)</code><br>获取或实例化一个SparkContext并将其注册为单例对象。</p>
<p>参数：conf – SparkConf (optional)</p>
<h4 id="startTime"><a href="#startTime" class="headerlink" title="startTime"></a>startTime</h4><p>返回SparkContext启动时的时间</p>
<h4 id="stop"><a href="#stop" class="headerlink" title="stop()"></a>stop()</h4><p>关闭SparkContext。</p>
<h4 id="range-start-end-None-step-1-numSlices-None"><a href="#range-start-end-None-step-1-numSlices-None" class="headerlink" title="range(start, end=None,step = 1, numSlices = None)"></a>range(start, end=None,step = 1, numSlices = None)</h4><p>创建一个新的包含从start到end元素的RDD，每个元素按步长增加。 可以像python的内置<code>range()</code>函数一样调用。 如果使用单个参数调用，则该参数被解释为<code>end</code>，<code>start</code>被设置为0。</p>
<p>Parameters:</p>
<ul>
<li>start – the start value</li>
<li>end – the end value (exclusive)</li>
<li>step – 步长(default: 1)</li>
<li>numSlices –新RDD的分区数</li>
</ul>
<p>Returns:           An RDD of int</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.range(5).collect()</span><br><span class="line">[0, 1, 2, 3, 4]</span><br><span class="line">&gt;&gt;&gt; sc.range(2, 4).collect()</span><br><span class="line">[2, 3]</span><br><span class="line">&gt;&gt;&gt; sc.range(1, 7, 2).collect()</span><br><span class="line">[1, 3, 5]</span><br></pre></td></tr></table></figure>
<h4 id="unions-rdds"><a href="#unions-rdds" class="headerlink" title="unions(rdds)"></a>unions(rdds)</h4><p>构建RDD列表的联合。</p>
<p>这支持使用不同序列化格式的RDD的unions()，尽管这迫使它们使用默认序列化程序进行反序列化：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#os库，还记得吗？</span><br><span class="line">&gt;&gt;&gt; path = os.path.join(tempdir, &quot;union-text.txt&quot;)</span><br><span class="line">&gt;&gt;&gt; with open(path, &quot;w&quot;) as testFile:</span><br><span class="line">...    _ = testFile.write(&quot;Hello&quot;)</span><br><span class="line">&gt;&gt;&gt; textFile = sc.textFile(path)</span><br><span class="line">&gt;&gt;&gt; textFile.collect()</span><br><span class="line">[u&apos;Hello&apos;]</span><br><span class="line">&gt;&gt;&gt; parallelized = sc.parallelize([&quot;World!&quot;])</span><br><span class="line">&gt;&gt;&gt; sorted(sc.union([textFile, parallelized]).collect())</span><br><span class="line">[u&apos;Hello&apos;, &apos;World!&apos;]</span><br></pre></td></tr></table></figure></p>
<h4 id="RDD的优先位置（preferredLocations）"><a href="#RDD的优先位置（preferredLocations）" class="headerlink" title="RDD的优先位置（preferredLocations）"></a>RDD的优先位置（preferredLocations）</h4><p>RDD优先位置属性与Spark中的调度有关，返回的是RDD的每个partition所存储的位置。</p>
<h3 id="操作RDD"><a href="#操作RDD" class="headerlink" title="操作RDD"></a>操作RDD</h3><p>形如RDD.XXX<br><code>sum()</code>用来对所有记录的长度求和<br><code>cache()</code>将数据缓存在内存里：<code>rddFromTextFile.cache</code></p>
<p><code>take(n)</code>返回RDD前n个元素</p>
<h4 id="partitionBy-numPartitions-parttionFunc-…"><a href="#partitionBy-numPartitions-parttionFunc-…" class="headerlink" title="partitionBy(numPartitions, parttionFunc=…)"></a>partitionBy(numPartitions, parttionFunc=…)</h4><p>返回用指定分区器进行分区的RDD的副本。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))</span><br><span class="line">&gt;&gt;&gt; sets = pairs.partitionBy(2).glom().collect()</span><br><span class="line">&gt;&gt;&gt; len(set(sets[0]).intersection(set(sets[1])))</span><br><span class="line">0</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># partitionBy</span><br><span class="line">x = sc.parallelize([(0,1),(1,2),(2,3)],2)</span><br><span class="line">y = x.partitionBy(numPartitions = 3, partitionFunc = lambda x: x)  # only key is passed to paritionFunc</span><br><span class="line">print(x.glom().collect())</span><br><span class="line">print(y.glom().collect())</span><br><span class="line"></span><br><span class="line">[[(0, 1)], [(1, 2), (2, 3)]]</span><br><span class="line">[[(0, 1)], [(1, 2)], [(2, 3)]]</span><br></pre></td></tr></table></figure>
<h4 id="repartition-numPartitions"><a href="#repartition-numPartitions" class="headerlink" title="repartition(numPartitions)"></a>repartition(numPartitions)</h4><p>返回一个正好具有numPartition分区的新RDD。<br>Reshuffle（重新洗牌）RDD 中的数据以创建或者更多的 partitions（分区）并将每个分区中的数据尽量保持均匀. 该操作总是通过网络来 shuffles 所有的数据.<br>可以增加或减少此RDD中的并行度。 在内部，它使用随机播放来重新分配数据。 如果您减少此RDD中的分区数，请考虑使用<code>coalesce</code>，这可以避免执行shuffle。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([1,2,3,4,5,6,7], 4)</span><br><span class="line">&gt;&gt;&gt; sorted(rdd.glom().collect())</span><br><span class="line">[[1], [2, 3], [4, 5], [6, 7]]</span><br><span class="line">&gt;&gt;&gt; len(rdd.repartition(2).glom().collect())</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; len(rdd.repartition(10).glom().collect())</span><br><span class="line">10</span><br></pre></td></tr></table></figure></p>
<h4 id="repaetitionAndSortWithPartitions-…"><a href="#repaetitionAndSortWithPartitions-…" class="headerlink" title="repaetitionAndSortWithPartitions( …)"></a>repaetitionAndSortWithPartitions( …)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">repaetitionAndSortWithPartitions(numPartitions=None, partitionFunc = ... , ascending = True , keyfunc = ...)</span><br></pre></td></tr></table></figure>
<p>根据给定的分区器重新分区RDD，并且在每个生成的分区中，通过它们的键对记录进行排序。这比每一个分区中先调用 <code>repartition</code> 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作的机器上进行.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])</span><br><span class="line">&gt;&gt;&gt; rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, 2)</span><br><span class="line">&gt;&gt;&gt; rdd2.glom().collect()</span><br><span class="line">[[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]</span><br></pre></td></tr></table></figure></p>
<h4 id="coalesce-numPartitions-shuffle-False"><a href="#coalesce-numPartitions-shuffle-False" class="headerlink" title="coalesce(numPartitions, shuffle = False)"></a>coalesce(numPartitions, shuffle = False)</h4><p>合并分区，返回一个减少到numPartition分区的新RDD。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()</span><br><span class="line">[[1], [2, 3], [4, 5]]</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()</span><br><span class="line">[[1, 2, 3, 4, 5]]</span><br></pre></td></tr></table></figure></p>
<h4 id="randomSplit-weights-seed-None"><a href="#randomSplit-weights-seed-None" class="headerlink" title="randomSplit(weights, seed = None)"></a>randomSplit(weights, seed = None)</h4><p>用提供的权重随机分割此RDD。<br><strong>参数</strong>：</p>
<pre><code>- 权重 ：分割权重，如果不等于1则将被归一化
- 种子 -：随机种子
</code></pre><p><strong>返回</strong>：<br>          拆分RDD为列表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize(range(500), 1)</span><br><span class="line">&gt;&gt;&gt; rdd1, rdd2 = rdd.randomSplit([2, 3], 17)</span><br><span class="line">&gt;&gt;&gt; len(rdd1.collect() + rdd2.collect())</span><br><span class="line">500</span><br><span class="line">&gt;&gt;&gt; 150 &lt; rdd1.count() &lt; 250</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; 250 &lt; rdd2.count() &lt; 350</span><br><span class="line">True</span><br></pre></td></tr></table></figure>
<h4 id="keyBy"><a href="#keyBy" class="headerlink" title="keyBy()"></a>keyBy()</h4><p>通过函数 f 来创建RDD中元素的元组。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)</span><br><span class="line">&gt;&gt;&gt; y = sc.parallelize(zip(range(0,5), range(0,5)))</span><br><span class="line">&gt;&gt;&gt; [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]</span><br><span class="line">[(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]</span><br></pre></td></tr></table></figure></p>
<h4 id="keys"><a href="#keys" class="headerlink" title="keys()"></a>keys()</h4><p>返回一个由每个元组的键组成的RDD。</p>
<h4 id="values"><a href="#values" class="headerlink" title="values()"></a>values()</h4><p>使用每个元组的值返回一个RDD。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; m = sc.parallelize([(1, 2), (3, 4)]).values()</span><br><span class="line">&gt;&gt;&gt; m.collect()</span><br><span class="line">[2, 4]</span><br></pre></td></tr></table></figure></p>
<h4 id="collect"><a href="#collect" class="headerlink" title="collect()"></a>collect()</h4><p><code>collect()</code>是一个Spark 的 action 函数， 将整个RDD以集合的形式返回驱动程序，通常在需要将结果返回到驱动程序坐在的节点以供本地处理时使用。<strong>注意</strong>：如果在非常大的数据集上使用可能耗尽内存。</p>
<h4 id="collectAsMap"><a href="#collectAsMap" class="headerlink" title="collectAsMap()"></a>collectAsMap()</h4><p>将RDD中的键值对返回给主机作为字典。同样，只有在生成的数据预期较小时，才应使用此方法，因为所有数据都将加载到驱动程序的内存中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()</span><br><span class="line">&gt;&gt;&gt; m[1]</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; m[3]</span><br><span class="line">4</span><br></pre></td></tr></table></figure></p>
<h4 id="persist-storageLevel-StorageLevel-False-True-False-False-1"><a href="#persist-storageLevel-StorageLevel-False-True-False-False-1" class="headerlink" title="persist(storageLevel=StorageLevel(False, True, False, False, 1))"></a>persist(storageLevel=StorageLevel(False, True, False, False, 1))</h4><p>设置此RDD的存储级别，以便在第一次计算后将其值跨操作持久化。 如果RDD尚未设置存储级别，则这只能用于分配新的存储级别。 如果没有指定存储级别默认值为（MEMORY_ONLY）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([&quot;b&quot;, &quot;a&quot;, &quot;c&quot;])</span><br><span class="line">&gt;&gt;&gt; rdd.persist().is_cached</span><br><span class="line">True</span><br></pre></td></tr></table></figure></p>
<h4 id="cache"><a href="#cache" class="headerlink" title="cache()"></a>cache()</h4><p>使用默认存储级别（MEMORY_ONLY）保持此RDD。</p>
<h4 id="count"><a href="#count" class="headerlink" title="count()"></a>count()</h4><p>action函数。<br>返回此RDD中的元素数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([2, 3, 4]).count()</span><br><span class="line">3</span><br></pre></td></tr></table></figure></p>
<h4 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h4><p>计算每个键的元素数量，并将结果作为字典返回到主数据。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])</span><br><span class="line">&gt;&gt;&gt; sorted(rdd.countByKey().items())</span><br><span class="line">[(&apos;a&apos;, 2), (&apos;b&apos;, 1)]</span><br></pre></td></tr></table></figure></p>
<h4 id="countByValue"><a href="#countByValue" class="headerlink" title="countByValue()"></a>countByValue()</h4><p>将此RDD中每个唯一值的计数作为（值，计数）对的字典返回。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())</span><br><span class="line">[(1, 2), (2, 3)]</span><br></pre></td></tr></table></figure></p>
<p>####distinct(numPartitions=None)<br>返回一个包含此RDD中不同元素的新RDD。就是去重。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())</span><br><span class="line">[1, 2, 3]</span><br></pre></td></tr></table></figure></p>
<h4 id="intersection-other"><a href="#intersection-other" class="headerlink" title="intersection(other)"></a>intersection(other)</h4><p>返回此RDD和另一个的交集，输出不包含任何重复的元素。</p>
<blockquote>
<p>注意This method performs a shuffle internally.（输出无序应该是）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])</span><br><span class="line">&gt;&gt;&gt; rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])</span><br><span class="line">&gt;&gt;&gt; rdd1.intersection(rdd2).collect()</span><br><span class="line">[1, 2, 3]</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h4 id="union-other"><a href="#union-other" class="headerlink" title="union(other)"></a>union(other)</h4><p>返回新RDD，包含这个RDD和其它RDD的并集。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 1, 2, 3])</span><br><span class="line">&gt;&gt;&gt; rdd.union(rdd).collect()</span><br><span class="line">[1, 1, 2, 3, 1, 1, 2, 3]</span><br></pre></td></tr></table></figure></p>
<h4 id="subtract-other-numPartitions-None"><a href="#subtract-other-numPartitions-None" class="headerlink" title="subtract(other, numPartitions=None)"></a>subtract(other, numPartitions=None)</h4><p>返回包含在自身但是不包含在 other 中的值。比较的是元素：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 4), (&quot;b&quot;, 5), (&quot;a&quot;, 3)])</span><br><span class="line">&gt;&gt;&gt; y = sc.parallelize([(&quot;a&quot;, 3), (&quot;c&quot;, None)])</span><br><span class="line">&gt;&gt;&gt; sorted(x.subtract(y).collect())</span><br><span class="line">[(&apos;a&apos;, 1), (&apos;b&apos;, 4), (&apos;b&apos;, 5)]</span><br></pre></td></tr></table></figure></p>
<h4 id="subtractByKey-other-numPartitions-None"><a href="#subtractByKey-other-numPartitions-None" class="headerlink" title="subtractByKey(other, numPartitions=None)"></a>subtractByKey(other, numPartitions=None)</h4><p>返回在自身中有而在 other 中没有匹配键对应的键值对。这里比较的是键：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 4), (&quot;b&quot;, 5), (&quot;a&quot;, 2)])</span><br><span class="line">&gt;&gt;&gt; y = sc.parallelize([(&quot;a&quot;, 3), (&quot;c&quot;, None)])</span><br><span class="line">&gt;&gt;&gt; sorted(x.subtractByKey(y).collect())</span><br><span class="line">[(&apos;b&apos;, 4), (&apos;b&apos;, 5)]</span><br></pre></td></tr></table></figure>
<h4 id="sum"><a href="#sum" class="headerlink" title="sum()"></a>sum()</h4><p>将此RDD中的元素相加。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([1.0, 2.0, 3.0]).sum()</span><br><span class="line">6.0</span><br></pre></td></tr></table></figure></p>
<h4 id="histogram-buckets"><a href="#histogram-buckets" class="headerlink" title="histogram(buckets)"></a>histogram(buckets)</h4><p>直方图<br>根据给定的区间（buckets）计算直方图，区间右边都是开的，比如 [1, 10, 20, 50] 指的是 [1, 10) [10, 20) [20, 50]。 And on the input of 1 and 50 we would have a histogram of 1,0,1。输入的区间必须是排序好的，不包含重复项，至少有两个元素。</p>
<p>返回值是 a tuple of buckets and histogram.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize(range(51))</span><br><span class="line">&gt;&gt;&gt; rdd.histogram(2)</span><br><span class="line">([0, 25, 50], [25, 26])</span><br><span class="line">&gt;&gt;&gt; rdd.histogram([0, 5, 25, 50])</span><br><span class="line">([0, 5, 25, 50], [5, 20, 26])</span><br><span class="line">&gt;&gt;&gt; rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets</span><br><span class="line">([0, 15, 30, 45, 60], [15, 15, 15, 6])</span><br><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([&quot;ab&quot;, &quot;ac&quot;, &quot;b&quot;, &quot;bd&quot;, &quot;ef&quot;])</span><br><span class="line">&gt;&gt;&gt; rdd.histogram((&quot;a&quot;, &quot;b&quot;, &quot;c&quot;))</span><br><span class="line">((&apos;a&apos;, &apos;b&apos;, &apos;c&apos;), [2, 2])</span><br></pre></td></tr></table></figure></p>
<h4 id="mean"><a href="#mean" class="headerlink" title="mean()"></a>mean()</h4><p>计算这个RDD元素的平均值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3]).mean()</span><br><span class="line">2.0</span><br></pre></td></tr></table></figure></p>
<h4 id="stdev"><a href="#stdev" class="headerlink" title="stdev()"></a>stdev()</h4><p>计算该RDD元素的标准差。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3]).stdev()</span><br><span class="line">0.816...</span><br></pre></td></tr></table></figure></p>
<h4 id="variance"><a href="#variance" class="headerlink" title="variance()"></a>variance()</h4><p>计算这个RDD中元素的方差。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3]).variance()</span><br><span class="line">0.666...</span><br></pre></td></tr></table></figure></p>
<h4 id="stats"><a href="#stats" class="headerlink" title="stats()"></a>stats()</h4><p>返回一个StatCounter object包含 mean, variance and count of the RDD’s elements in one operation.</p>
<h4 id="sample-withReplacement-fraction-seed-None"><a href="#sample-withReplacement-fraction-seed-None" class="headerlink" title="sample(withReplacement, fraction, seed=None)"></a>sample(withReplacement, fraction, seed=None)</h4><p>返回此RDD的采样子集。<br><strong>参数</strong>：</p>
<ul>
<li><p>withReplacement： 可以对元素进行多次采样（是否放回）</p>
</li>
<li><p>fraction ：样本的预期大小作为without替换的RDD大小的分数：每个元素被选择的概率; fraction必须在[0，1]with替换：每个元素选择的预期次数; 分数必须&gt; = 0（采样的百分比）</p>
</li>
<li><p>seed： 随机数发生器的种子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize(range(100), 4)</span><br><span class="line">&gt;&gt;&gt; 6 &lt;= rdd.sample(False, 0.1, 81).count() &lt;= 14</span><br><span class="line">True</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># sample</span><br><span class="line">x = sc.parallelize(range(7))</span><br><span class="line"># call &apos;sample&apos; 5 times</span><br><span class="line">ylist = [x.sample(withReplacement=False, fraction=0.5) for i in range(5)]</span><br><span class="line">print(&apos;x = &apos; + str(x.collect()))</span><br><span class="line">for cnt, y in zip(range(len(ylist)), ylist):</span><br><span class="line">    print(&apos;sample:&apos; + str(cnt) + &apos; y = &apos; +  str(y.collect()))</span><br><span class="line"></span><br><span class="line">x = [0, 1, 2, 3, 4, 5, 6]</span><br><span class="line">sample:0 y = [0, 2, 5, 6]</span><br><span class="line">sample:1 y = [2, 6]</span><br><span class="line">sample:2 y = [0, 4, 5, 6]</span><br><span class="line">sample:3 y = [0, 2, 6]</span><br><span class="line">sample:4 y = [0, 3, 4]</span><br></pre></td></tr></table></figure>
<h4 id="sampleByKey-withPrplacement-fractions-seed-None"><a href="#sampleByKey-withPrplacement-fractions-seed-None" class="headerlink" title="sampleByKey(withPrplacement, fractions, seed = None)"></a>sampleByKey(withPrplacement, fractions, seed = None)</h4><p>返回通过键采样的该RDD的子集（通过分层采样）。 使用分数指定的不同键的可变采样率创建此RDD的样本，a key to sampling rate map.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; fractions = &#123;&quot;a&quot;: 0.2, &quot;b&quot;: 0.1&#125;</span><br><span class="line">&gt;&gt;&gt; rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))</span><br><span class="line">&gt;&gt;&gt; sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())</span><br><span class="line">&gt;&gt;&gt; 100 &lt; len(sample[&quot;a&quot;]) &lt; 300 and 50 &lt; len(sample[&quot;b&quot;]) &lt; 150</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; max(sample[&quot;a&quot;]) &lt;= 999 and min(sample[&quot;a&quot;]) &gt;= 0</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; max(sample[&quot;b&quot;]) &lt;= 999 and min(sample[&quot;b&quot;]) &gt;= 0</span><br><span class="line">True</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># sampleByKey</span><br><span class="line">x = sc.parallelize([(&apos;A&apos;,1),(&apos;B&apos;,2),(&apos;C&apos;,3),(&apos;B&apos;,4),(&apos;A&apos;,5)])</span><br><span class="line">y = x.sampleByKey(withReplacement=False, fractions=&#123;&apos;A&apos;:0.5, &apos;B&apos;:1, &apos;C&apos;:0.2&#125;)</span><br><span class="line">print(x.collect())</span><br><span class="line">print(y.collect())</span><br><span class="line"></span><br><span class="line">[(&apos;A&apos;, 1), (&apos;B&apos;, 2), (&apos;C&apos;, 3), (&apos;B&apos;, 4), (&apos;A&apos;, 5)]</span><br><span class="line">[(&apos;B&apos;, 2), (&apos;C&apos;, 3), (&apos;B&apos;, 4)]</span><br></pre></td></tr></table></figure>
<h4 id="sampleStdev"><a href="#sampleStdev" class="headerlink" title="sampleStdev()"></a>sampleStdev()</h4><p>计算该RDD元素的样本标准偏差（其通过除以N-1而不是N来校正标准偏差中的偏差）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3]).sampleStdev()</span><br><span class="line">1.0</span><br></pre></td></tr></table></figure></p>
<h4 id="sampleVariance"><a href="#sampleVariance" class="headerlink" title="sampleVariance()"></a>sampleVariance()</h4><p>计算这个RDD元素的样本方差（通过用N-1而不是N来估计方差来校正偏差）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3]).sampleVariance()</span><br><span class="line">1.0</span><br></pre></td></tr></table></figure></p>
<h4 id="zipWithIndex"><a href="#zipWithIndex" class="headerlink" title="zipWithIndex()"></a>zipWithIndex()</h4><p>使用其元素索引将此RDD压缩。</p>
<p>排序首先基于分区索引，然后根据每个分区中的项目顺序。 因此，第一个分区中的第一个项目获取索引0，最后一个分区中的最后一个项目将收到最大的索引。</p>
<p>当此RDD包含多个分区时，此方法需要触发spark job。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;], 3).zipWithIndex().collect()</span><br><span class="line">[(&apos;a&apos;, 0), (&apos;b&apos;, 1), (&apos;c&apos;, 2), (&apos;d&apos;, 3)]</span><br></pre></td></tr></table></figure></p>
<h4 id="zipWithUniqueId"><a href="#zipWithUniqueId" class="headerlink" title="zipWithUniqueId()"></a>zipWithUniqueId()</h4><p>使用生成的唯一长 ID 来zip此 RDD。</p>
<p>第 k 个分区中的项将得到 ID k，n + k，2 * n + k，…，其中n是分区数。 所以可能存在差距，但是这种方法不会引发spark job，这与zipWithIndex不同。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;], 3).zipWithUniqueId().collect()</span><br><span class="line">[(&apos;a&apos;, 0), (&apos;b&apos;, 1), (&apos;c&apos;, 4), (&apos;d&apos;, 2), (&apos;e&apos;, 5)]</span><br></pre></td></tr></table></figure></p>
<h4 id="sortBy-keyfunc-ascending-True-numPartitions-None"><a href="#sortBy-keyfunc-ascending-True-numPartitions-None" class="headerlink" title="sortBy(keyfunc, ascending=True, numPartitions=None)"></a>sortBy(keyfunc, ascending=True, numPartitions=None)</h4><p>通过给定的keyfunc对此RDD进行排序。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; tmp = [(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;1&apos;, 3), (&apos;d&apos;, 4), (&apos;2&apos;, 5)]</span><br><span class="line">&gt;&gt;&gt; sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()</span><br><span class="line">[(&apos;1&apos;, 3), (&apos;2&apos;, 5), (&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;d&apos;, 4)]</span><br><span class="line">&gt;&gt;&gt; sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()</span><br><span class="line">[(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;1&apos;, 3), (&apos;d&apos;, 4), (&apos;2&apos;, 5)]</span><br></pre></td></tr></table></figure></p>
<h4 id="sortByKey-ascending-True-numPartitions-None-keyfunc-…"><a href="#sortByKey-ascending-True-numPartitions-None-keyfunc-…" class="headerlink" title="sortByKey(ascending=True, numPartitions=None, keyfunc=…)"></a>sortByKey(ascending=True, numPartitions=None, keyfunc=…)</h4><p>在一个 (K, V) pair 的 dataset 上调用时，其中的 K 实现了 Ordered，返回一个按 keys 升序或降序的 (K, V) pairs 的 dataset, 由 boolean 类型的 <code>ascending</code> 参数来指定.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; tmp = [(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;1&apos;, 3), (&apos;d&apos;, 4), (&apos;2&apos;, 5)]</span><br><span class="line">&gt;&gt;&gt; sc.parallelize(tmp).sortByKey().first()</span><br><span class="line">(&apos;1&apos;, 3)</span><br><span class="line">&gt;&gt;&gt; sc.parallelize(tmp).sortByKey(True, 1).collect()</span><br><span class="line">[(&apos;1&apos;, 3), (&apos;2&apos;, 5), (&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;d&apos;, 4)]</span><br><span class="line">&gt;&gt;&gt; sc.parallelize(tmp).sortByKey(True, 2).collect()</span><br><span class="line">[(&apos;1&apos;, 3), (&apos;2&apos;, 5), (&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;d&apos;, 4)]</span><br><span class="line">&gt;&gt;&gt; tmp2 = [(&apos;Mary&apos;, 1), (&apos;had&apos;, 2), (&apos;a&apos;, 3), (&apos;little&apos;, 4), (&apos;lamb&apos;, 5)]</span><br><span class="line">&gt;&gt;&gt; tmp2.extend([(&apos;whose&apos;, 6), (&apos;fleece&apos;, 7), (&apos;was&apos;, 8), (&apos;white&apos;, 9)])</span><br><span class="line">&gt;&gt;&gt; sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()</span><br><span class="line">[(&apos;a&apos;, 3), (&apos;fleece&apos;, 7), (&apos;had&apos;, 2), (&apos;lamb&apos;, 5),...(&apos;white&apos;, 9), (&apos;whose&apos;, 6)]</span><br></pre></td></tr></table></figure></p>
<h4 id="glom"><a href="#glom" class="headerlink" title="glom()"></a>glom()</h4><p>一个分区的元素放在同一个list中，作为新RDD返回。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 2)</span><br><span class="line">&gt;&gt;&gt; sorted(rdd.glom().collect())</span><br><span class="line">[[1, 2], [3, 4]]</span><br></pre></td></tr></table></figure></p>
<h4 id="foreach-f"><a href="#foreach-f" class="headerlink" title="foreach(f)"></a>foreach(f)</h4><p>对这个RDD的所有元素应用一个函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def f(x): print(x)</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).foreach(f)</span><br></pre></td></tr></table></figure></p>
<h4 id="foreachPartition-f"><a href="#foreachPartition-f" class="headerlink" title="foreachPartition(f)"></a>foreachPartition(f)</h4><p>将函数应用于此RDD的每个分区。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def f(iterator):</span><br><span class="line">...     for x in iterator:</span><br><span class="line">...          print(x)</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)</span><br></pre></td></tr></table></figure></p>
<h4 id="pipe-command-env-None-checkCode-False"><a href="#pipe-command-env-None-checkCode-False" class="headerlink" title="pipe(command, env=None, checkCode=False)"></a>pipe(command, env=None, checkCode=False)</h4><p>pipe管道将pyspark的RDD对象fork给外部进程，然后再返回RDD对象。<br>通过使用 shell 命令来将每个 RDD 的分区给 Pipe。例如，一个 Perl 或 bash 脚本。RDD 的元素会被写入进程的标准输入（stdin），并且 lines（行）输出到它的标准输出（stdout）被作为一个字符串型 RDD 的 string 返回.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># pipe</span><br><span class="line">x = sc.parallelize([&apos;A&apos;, &apos;Ba&apos;, &apos;C&apos;, &apos;AD&apos;])</span><br><span class="line">y = x.pipe(&apos;grep -i &quot;A&quot;&apos;) # calls out to grep, may fail under Windows</span><br><span class="line">print(x.collect())</span><br><span class="line">print(y.collect())</span><br><span class="line"></span><br><span class="line">[&apos;A&apos;, &apos;Ba&apos;, &apos;C&apos;, &apos;AD&apos;]</span><br><span class="line">[&apos;A&apos;, &apos;Ba&apos;, &apos;AD&apos;]</span><br></pre></td></tr></table></figure></p>
<h4 id="cartesian-other"><a href="#cartesian-other" class="headerlink" title="cartesian(other)"></a>cartesian(other)</h4><p>返回此RDD和另一个RDD的笛卡尔乘积，即所有成对元素（a，b）的RDD，其中a为自身，b为其他元素。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 2])</span><br><span class="line">&gt;&gt;&gt; sorted(rdd.cartesian(rdd).collect())</span><br><span class="line">[(1, 1), (1, 2), (2, 1), (2, 2)]</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>在数学中，两个集合X和Y的笛卡儿积（Cartesian product），又称直积，在集合论中表示为X × Y，是所有可能的有序对組成的集合，其中有序对的第一个对象是X的成员，第二个对象是Y的成员。</p>
</blockquote>
<h4 id="aggregate-zeroValue-seqOp-combOp"><a href="#aggregate-zeroValue-seqOp-combOp" class="headerlink" title="aggregate(zeroValue, seqOp, combOp)"></a>aggregate(zeroValue, seqOp, combOp)</h4><p>聚合每个分区的元素，然后对所有分区的结果，使用给定的组合函数和中性“零值”。允许函数op（t1，t2）修改t1并返回它作为其结果值，以避免对象分配; 但是，它不应该修改t2。<br>第一个函数（seqOp）可以返回与该RDD类型不同的结果类型U。 因此，我们需要一个用于将T合并成U的操作和用于合并两个U的一个操作。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#建立各分区内的聚集函数，由初始值依次与分区内的函数做操作</span><br><span class="line">&gt;&gt;&gt; seqOp = (lambda x, y: (x[0] + y, x[1] + 1))</span><br><span class="line">#建立各分区间的组合函数，</span><br><span class="line">&gt;&gt;&gt; combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))</span><br><span class="line">#样例1</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4], 4).aggregate((0, 0), seqOp, combOp)</span><br><span class="line">(10, 4)</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([]).aggregate((0, 0), seqOp, combOp)</span><br><span class="line">(0, 0)</span><br><span class="line">#样例2</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([ 1, 2, 3, 4, 5]，3).aggregate((1,1), seqOp, combOp)</span><br><span class="line">(19,9）</span><br></pre></td></tr></table></figure></p>
<p><em>解释样例1</em>：<br>分区数：4<br>0：1<br>1：2<br>2：3<br>3：4<br>利用<code>zerovalue(0, 0)</code>和<code>seqOp</code>对各分区进行聚集：<br>x[0] = 0, x[1] = 0<br>0：(0+1, 0+1)-&gt;(1, 1)<br>1：(0+2, 0+1)-&gt;(2, 1)<br>2：(0+3, 0+1)-&gt;(3, 1)<br>3：(0+4, 0+1)-&gt;(4, 1)</p>
<p>利用<code>zerovalue(0, 0)</code>和<code>combOp</code>进行各分区间的聚合：<br>(0, 0) + (1, 1) + (2, 1) + (3, 1) + (4, 1) = (10, 4)</p>
<p><em>解释样例2</em>：<br>分区数：3<br>0：1, 2<br>1：3, 4<br>3：5</p>
<p>利用<code>zerovalue(1, 1)</code>和<code>seqOp</code>对各分区进行聚集：<br>x[0] = 1, x[1] = 1<br>0：(1+1, 1+1)=&gt;(2, 2)-&gt;x[0] = 2, x[1] = 2, y=2=&gt;(4, 3)<br>1：(1, 1) + (3, 1) + (4, 1) = (8, 3)<br>2：(1, 1) + (5, 1) = (6, 2)</p>
<p>利用<code>zerovalue(1, 1)</code>和<code>combOp</code>进行各分区间的聚合：<br>(1, 1) + (4, 3) + (8, 3) = (19, 9)</p>
<h4 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h4><p>使用给定的组合函数和中性“零值”来聚合每个键的值。 该函数可以返回不同于此RDD，V中的值的类型的不同的结果类型U。因此，我们需要一个操作来将V合并到一个U中，一个操作用于合并两个U，前者的操作用于 合并分区中的值，后者用于在分区之间合并值。 为了避免内存分配，这两个函数都允许修改并返回其第一个参数，而不是创建一个新的U.</p>
<p>在 (K, V) pairs 的 dataset 上调用时, 返回 (K, U) pairs 的 dataset，其中的 values 是针对每个 key 使用给定的 combine 函数以及一个 neutral “0” 值来进行聚合的. 允许聚合值的类型与输入值的类型不一样, 同时避免不必要的配置. 像 groupByKey 一样, reduce tasks 的数量是可以通过第二个可选的参数来配置的。</p>
<p><code>aggregateByKey</code> 把类型为KV的RDD类型转为K U的RDD，V和U的类型可以不一样，这一点跟<code>combineByKey</code>是一样的，即返回的二元组值类型可以不一样</p>
<p><code>aggregateByKey</code> 内部是通过调用combineByKey来实现的，<code>combineByKey</code>的<code>createCombine</code>函数逻辑由<code>zeroValue</code>这个变量实现，<code>zeroValue</code> 作为聚合的初始值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># aggregateByKey</span><br><span class="line">x = sc.parallelize([(&apos;B&apos;,1),(&apos;B&apos;,2),(&apos;A&apos;,3),(&apos;A&apos;,4),(&apos;A&apos;,5)])</span><br><span class="line">zeroValue = [] # empty list is &apos;zero value&apos; for append operation</span><br><span class="line">mergeVal = (lambda aggregated, el: aggregated + [(el,el**2)])</span><br><span class="line">mergeComb = (lambda agg1,agg2: agg1 + agg2 )</span><br><span class="line">y = x.aggregateByKey(zeroValue,mergeVal,mergeComb)</span><br><span class="line">print(x.collect())</span><br><span class="line">print(y.collect())</span><br><span class="line"></span><br><span class="line">[(&apos;B&apos;, 1), (&apos;B&apos;, 2), (&apos;A&apos;, 3), (&apos;A&apos;, 4), (&apos;A&apos;, 5)]</span><br><span class="line">[(&apos;A&apos;, [(3, 9), (4, 16), (5, 25)]), (&apos;B&apos;, [(1, 1), (2, 4)])]</span><br></pre></td></tr></table></figure>
<h4 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupBy(f, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f51f1ac0668&gt;)</span><br></pre></td></tr></table></figure>
<p>返回根据给定函数分好组的RDD。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 1, 2, 3, 5, 8])</span><br><span class="line">&gt;&gt;&gt; result = rdd.groupBy(lambda x: x % 2).collect()</span><br><span class="line">&gt;&gt;&gt; sorted([(x, sorted(y)) for (x, y) in result])</span><br><span class="line">[(0, [2, 8]), (1, [1, 1, 3, 5])]</span><br></pre></td></tr></table></figure></p>
<h4 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupByKey(numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f51f1ac0668&gt;)</span><br></pre></td></tr></table></figure>
<p>将单个RDD中每个键的值分组为单个序列。在一个 (K, V) pair 的 dataset 上调用时，返回一个 (K, Iterable<v>)。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])</span><br><span class="line">&gt;&gt;&gt; sorted(rdd.groupByKey().mapValues(len).collect())</span><br><span class="line">[(&apos;a&apos;, 2), (&apos;b&apos;, 1)]</span><br><span class="line">&gt;&gt;&gt; sorted(rdd.groupByKey().mapValues(list).collect())</span><br><span class="line">[(&apos;a&apos;, [1, 1]), (&apos;b&apos;, [1])]</span><br></pre></td></tr></table></figure></v></p>
<blockquote>
<p><strong>注意</strong>：注意如果为了在每个key上执行聚合操作（如sum或average）进行分组，则使用<code>reduceByKey</code>或<code>aggregateByKey</code>将提供更好的性能。<br><strong>注意</strong>：默认情况下，并行度取决于父 RDD 的分区数。可以传递一个可选的 numTasks 参数来设置不同的任务数。</p>
</blockquote>
<h4 id="cogroup-other-numPartitions-None"><a href="#cogroup-other-numPartitions-None" class="headerlink" title="cogroup(other, numPartitions=None)"></a>cogroup(other, numPartitions=None)</h4><p>两个RDD元素一起分组，对于自身或其他中的每个关键字k，返回包含元组的结果RDD，该元组具有该键在自身以及其他RDD中对应的值的<em>列表</em>。<br>在一个 (K, V) 和的 dataset 上调用时，返回一个 <code>(K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples</code> 的 dataset. 这个操作也调用了 <code>groupWith</code>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 4)])</span><br><span class="line">&gt;&gt;&gt; y = sc.parallelize([(&quot;a&quot;, 2)])</span><br><span class="line">#列表推导式</span><br><span class="line">&gt;&gt;&gt; [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]</span><br><span class="line">[(&apos;a&apos;, ([1], [2])), (&apos;b&apos;, ([4], []))]</span><br></pre></td></tr></table></figure></p>
<h4 id="groupWith-other-others"><a href="#groupWith-other-others" class="headerlink" title="groupWith(other, *others)"></a>groupWith(other, *others)</h4><p>别名为cogroup但支持多个RDD。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; w = sc.parallelize([(&quot;a&quot;, 5), (&quot;b&quot;, 6)])</span><br><span class="line">&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 4)])</span><br><span class="line">&gt;&gt;&gt; y = sc.parallelize([(&quot;a&quot;, 2)])</span><br><span class="line">&gt;&gt;&gt; z = sc.parallelize([(&quot;b&quot;, 42)])</span><br><span class="line">&gt;&gt;&gt; [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]</span><br><span class="line">[(&apos;a&apos;, ([5], [1], [2], [])), (&apos;b&apos;, ([6], [4], [], [42]))]</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># groupWith</span><br><span class="line">x = sc.parallelize([(&apos;C&apos;,4),(&apos;B&apos;,(3,3)),(&apos;A&apos;,2),(&apos;A&apos;,(1,1))])</span><br><span class="line">y = sc.parallelize([(&apos;B&apos;,(7,7)),(&apos;A&apos;,6),(&apos;D&apos;,(5,5))])</span><br><span class="line">z = sc.parallelize([(&apos;D&apos;,9),(&apos;B&apos;,(8,8))])</span><br><span class="line">a = x.groupWith(y,z)</span><br><span class="line">print(x.collect())</span><br><span class="line">print(y.collect())</span><br><span class="line">print(z.collect())</span><br><span class="line">print(&quot;Result:&quot;)</span><br><span class="line">for key,val in list(a.collect()):</span><br><span class="line">    print(key, [list(i) for i in val])</span><br><span class="line"></span><br><span class="line">[(&apos;C&apos;, 4), (&apos;B&apos;, (3, 3)), (&apos;A&apos;, 2), (&apos;A&apos;, (1, 1))]</span><br><span class="line">[(&apos;B&apos;, (7, 7)), (&apos;A&apos;, 6), (&apos;D&apos;, (5, 5))]</span><br><span class="line">[(&apos;D&apos;, 9), (&apos;B&apos;, (8, 8))]</span><br><span class="line">Result:</span><br><span class="line">D [[], [(5, 5)], [9]]</span><br><span class="line">C [[4], [], []]</span><br><span class="line">B [[(3, 3)], [(7, 7)], [(8, 8)]]</span><br><span class="line">A [[2, (1, 1)], [6], []]</span><br></pre></td></tr></table></figure>
<h4 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f51f1ac0668&gt;)</span><br></pre></td></tr></table></figure>
<p>使用一组自定义聚合函数来组合每个键的元素。<br>将RDD[(K, V)]转换为RDD[(K, C)] ，C是<code>combined type</code>。<br>用户应该提供：</p>
<ul>
<li><p>createCombiner将V转换为C（例如，创建单元列表）</p>
</li>
<li><p>mergeValue，将V合并为C（例如，将其添加到列表的末尾）</p>
</li>
<li><p>mergeCombiners将两个C组合成一个C（例如，合并列表）</p>
</li>
</ul>
<p>为了避免内存分配，mergeValue和mergeCombiners都允许修改并返回其第一个参数，而不是创建一个新的C.<br>此外，用户可以控制输出RDD的分区。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 2)])</span><br><span class="line">&gt;&gt;&gt; def to_list(a):</span><br><span class="line">...     return [a]</span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt; def append(a, b):</span><br><span class="line">...     a.append(b)</span><br><span class="line">...     return a</span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt; def extend(a, b):</span><br><span class="line">...     a.extend(b)</span><br><span class="line">...     return a</span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt; sorted(x.combineByKey(to_list, append, extend).collect())</span><br><span class="line">[(&apos;a&apos;, [1, 2]), (&apos;b&apos;, [1])]</span><br></pre></td></tr></table></figure></p>
<h4 id="join-other-numPartitions-None"><a href="#join-other-numPartitions-None" class="headerlink" title="join(other, numPartitions = None)"></a>join(other, numPartitions = None)</h4><p>相当于inner join. 对两个需要连接的RDD进行cogroup，然后对每个Key下面的list进行笛卡尔积的操作，输出<br>两两相交两个集合作为value ，相当于sql中where a.key= b.key。<br>每对元素将作为(k, (v1, v2))元组返回，其中( k, v1)在自身中，(k, v2)在另一个元素中。</p>
<p>在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，它拥有每个 key 中所有的元素对。Outer joins 可以通过 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 来实现。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 4)])</span><br><span class="line">&gt;&gt;&gt; y = sc.parallelize([(&quot;a&quot;, 2), (&quot;a&quot;, 3)])</span><br><span class="line">&gt;&gt;&gt; sorted(x.join(y).collect())</span><br><span class="line">[(&apos;a&apos;, (1, 2)), (&apos;a&apos;, (1, 3))]</span><br></pre></td></tr></table></figure></p>
<h4 id="fullOuterJoin-other-numPartitions-None"><a href="#fullOuterJoin-other-numPartitions-None" class="headerlink" title="fullOuterJoin(other, numPartitions = None)"></a>fullOuterJoin(other, numPartitions = None)</h4><p> 实现<strong>self</strong>和<strong>other</strong>的全外联结<br>对于每个<strong>self</strong>中的 (k, v) 元素，结果RDD中将要么包含所有 (k,(v,w)) 对，w 来自<strong>other</strong>，要么为 (k, (v, None))，如果 <strong>other</strong>中没有 k 键。类似的，如果<strong>self</strong>中没有 k 键，就是 (k,(None, w))。<br>Hash-partitions the resulting RDD into the given number of partitions.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 4)])</span><br><span class="line">&gt;&gt;&gt; y = sc.parallelize([(&quot;a&quot;, 2), (&quot;c&quot;, 8)])</span><br><span class="line">&gt;&gt;&gt; sorted(x.fullOuterJoin(y).collect())</span><br><span class="line">[(&apos;a&apos;, (1, 2)), (&apos;b&apos;, (4, None)), (&apos;c&apos;, (None, 8))]</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>于 ANSI 标准的 SQL 列出了五种 JOIN 方式: 内连接(INNER), 全外连接(FULL OUTER), 左外连接(LEFT OUTER), 右外连接(RIGHT OUTER)和交叉连接(CROSS).外连接并不要求连接的两表的每一条记录在对方表中都一条匹配的记录。</p>
</blockquote>
<h4 id="leftOuterJoin-other-numPartitions-None"><a href="#leftOuterJoin-other-numPartitions-None" class="headerlink" title="leftOuterJoin(other, numPartitions=None)"></a>leftOuterJoin(other, numPartitions=None)</h4><h4 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h4><h4 id="fold-zeroValue-op"><a href="#fold-zeroValue-op" class="headerlink" title="fold(zeroValue, op)"></a>fold(zeroValue, op)</h4><p>聚合每个分区的元素，然后使用给定的关联函数和中性“零值”对所有分区的结果进行聚合。</p>
<p>允许函数op（t1，t2）修改t1并将其作为其结果值返回，以避免对象分配; 但是，它不应该修改t2。</p>
<p>这与Scala等功能语言中的非分布式集合实现的折叠操作有所不同。 该折叠操作可以单独应用于分区，然后将这些结果折叠成最终结果，而不是以某些定义的顺序将折叠应用于每个元素。 对于不可交换的功能，结果可能与应用于非分布式集合的折叠的结果不同。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from operator import add</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)</span><br><span class="line">15</span><br></pre></td></tr></table></figure></p>
<h4 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h4><p>使用关联函数“func”和中性“零值”合并每个键的值，该值可以添加到任意次数的结果中，并且不得更改结果（例如，0为加法，或1为乘法。）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])</span><br><span class="line">&gt;&gt;&gt; from operator import add</span><br><span class="line">&gt;&gt;&gt; sorted(rdd.foldByKey(0, add).collect())</span><br><span class="line">[(&apos;a&apos;, 2), (&apos;b&apos;, 1)]</span><br></pre></td></tr></table></figure></p>
<h4 id="map-f-preservesPartitioning-False-和flatmap"><a href="#map-f-preservesPartitioning-False-和flatmap" class="headerlink" title="map(f, preservesPartitioning = False)和flatmap"></a>map(f, preservesPartitioning = False)和flatmap</h4><p><code>map</code>是最常用的转换操作，对RDD中的每个元素都执行一个指定的函数来产生一个新的RDD。任何原RDD中的元素在新RDD中都有且只有一个元素与之对应。<br>而<code>flatmap</code>通过对这个RDD的所有元素应用一个函数，然后平坦化结果来返回一个新的RDD。所以 <code>func</code> 应该返回一个 <code>Seq</code> 而不是一个单独的 <code>item</code>。</p>
<p>Example below：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#from pyspark import SparkContext, SparkConf</span><br><span class="line">#sc = SparkContext(&quot;local&quot;,&quot;testApp&quot;)</span><br><span class="line"></span><br><span class="line">x = sc.parallelize([1,2,3])</span><br><span class="line">y = x.map(lambda x : (x, x**2))</span><br><span class="line">z = x.flatMap(lambda x: (x, 100*x, x**2))</span><br><span class="line"></span><br><span class="line">print(x.collect())</span><br><span class="line">print(y.collect())</span><br><span class="line">print(z.collect())</span><br></pre></td></tr></table></figure></p>
<p>out:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[1, 2, 3]</span><br><span class="line">[(1, 1), (2, 4), (3, 9)]</span><br><span class="line">[1, 100, 1, 2, 200, 4, 3, 300, 9]</span><br></pre></td></tr></table></figure></p>
<h4 id="flatMapValues-f"><a href="#flatMapValues-f" class="headerlink" title="flatMapValues(f)"></a>flatMapValues(f)</h4><p>传递键值对RDD中的每个值通过flatMap函数，而无需更改键; 这也保留了原始RDD的分区。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, [&quot;x&quot;, &quot;y&quot;, &quot;z&quot;]), (&quot;b&quot;, [&quot;p&quot;, &quot;r&quot;])])</span><br><span class="line">&gt;&gt;&gt; def f(x): return x</span><br><span class="line">&gt;&gt;&gt; x.flatMapValues(f).collect()</span><br><span class="line">[(&apos;a&apos;, &apos;x&apos;), (&apos;a&apos;, &apos;y&apos;), (&apos;a&apos;, &apos;z&apos;), (&apos;b&apos;, &apos;p&apos;), (&apos;b&apos;, &apos;r&apos;)]</span><br></pre></td></tr></table></figure></p>
<h4 id="mapvalues-f"><a href="#mapvalues-f" class="headerlink" title="mapvalues(f)"></a>mapvalues(f)</h4><p>只把键值对中的值进行map，不更改键和分区。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, [&quot;apple&quot;, &quot;banana&quot;, &quot;lemon&quot;]), (&quot;b&quot;, [&quot;grapes&quot;])])</span><br><span class="line">&gt;&gt;&gt; def f(x): return len(x)</span><br><span class="line">&gt;&gt;&gt; x.mapValues(f).collect()</span><br><span class="line">[(&apos;a&apos;, 3), (&apos;b&apos;, 1)]</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># mapValues</span><br><span class="line">x = sc.parallelize([(&apos;A&apos;, (1, 2, 3)), (&apos;B&apos;, (4, 5))])</span><br><span class="line">y = x.mapValues(lambda x: [i**2 for i in x]) # function is applied to entire value</span><br><span class="line">print(x.collect())</span><br><span class="line">print(y.collect())</span><br><span class="line"></span><br><span class="line">[(&apos;A&apos;, (1, 2, 3)), (&apos;B&apos;, (4, 5))]</span><br><span class="line">[(&apos;A&apos;, [1, 4, 9]), (&apos;B&apos;, [16, 25])]</span><br></pre></td></tr></table></figure>
<h4 id="mapPartitions-f-preservesPartitioning-False"><a href="#mapPartitions-f-preservesPartitioning-False" class="headerlink" title="mapPartitions(f, preservesPartitioning=False)"></a>mapPartitions(f, preservesPartitioning=False)</h4><p><code>mapPartitions</code>是<code>map</code>的一个变种,map的输入函数应用于RDD中的每一个元素，而<code>mapPartition</code>的输入函数是应用于每个分区，也就是把每个分区中的内容作为整体来处理。在一个类型为 T 的 RDD 上运行时 func 必须是 <code>Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;</code> 类型。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># mapPartitions</span><br><span class="line">x = sc.parallelize([1,2,3], 2)</span><br><span class="line">def f(iterator): yield sum(iterator)</span><br><span class="line">y = x.mapPartitions(f)</span><br><span class="line"># glom() flattens elements on the same partition</span><br><span class="line">print(x.glom().collect())</span><br><span class="line">print(y.glom().collect())</span><br><span class="line"></span><br><span class="line">[[1], [2, 3]]</span><br><span class="line">[[1], [5]]</span><br></pre></td></tr></table></figure></p>
<h4 id="mapPartitionsWithIndex-f-preservesPartitioning-False"><a href="#mapPartitionsWithIndex-f-preservesPartitioning-False" class="headerlink" title="mapPartitionsWithIndex(f, preservesPartitioning=False)"></a>mapPartitionsWithIndex(f, preservesPartitioning=False)</h4><p>通过对该RDD的每个分区应用一个函数，同时跟踪原始分区的索引来返回一个新的RDD。需要提供一个代表 partition 的 index（索引）的 interger value（整型值）作为参数的 <code>func</code>，所以在一个类型为 T 的 RDD 上运行时 func 必须是 <code>(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;</code> 类型。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 4)</span><br><span class="line">&gt;&gt;&gt; def f(splitIndex, iterator): yield splitIndex</span><br><span class="line">&gt;&gt;&gt; rdd.mapPartitionsWithIndex(f).sum()</span><br><span class="line">6</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># mapPartitionsWithIndex</span><br><span class="line">x = sc.parallelize([1,2,3], 2)</span><br><span class="line">def f(partitionIndex, iterator): yield (partitionIndex,sum(iterator))</span><br><span class="line">y = x.mapPartitionsWithIndex(f)</span><br><span class="line"></span><br><span class="line"># glom() flattens elements on the same partition</span><br><span class="line">print(x.glom().collect())</span><br><span class="line">print(y.glom().collect())</span><br><span class="line"></span><br><span class="line">[[1], [2, 3]]</span><br><span class="line">[[(0, 1)], [(1, 5)]]</span><br></pre></td></tr></table></figure>
<h4 id="filter-f"><a href="#filter-f" class="headerlink" title="filter(f)"></a>filter(f)</h4><p>返回一个仅包含满足谓词的元素的新RDD。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4, 5])</span><br><span class="line">&gt;&gt;&gt; rdd.filter(lambda x: x % 2 == 0).collect()</span><br><span class="line">[2, 4]</span><br></pre></td></tr></table></figure></p>
<h4 id="reduce-f"><a href="#reduce-f" class="headerlink" title="reduce(f)"></a>reduce(f)</h4><p>属于action。<br>Reduce比groupby好，能用reduce就用reduce。<br>reduce将RDD中元素两两传递给输入函数，同时产生一个新的值，新产生的值与RDD中下一个元素再被传递给输入函数，知道最后有一个值为止。<br>使用指定的交换和关联二进制运算符 reduce RDD的元素。只在本分区本地地进行reduce。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from operator import add</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).reduce(add)</span><br><span class="line">15</span><br></pre></td></tr></table></figure></p>
<h4 id="reduceByKey-func-numPartitions-None-parititonFunc-…"><a href="#reduceByKey-func-numPartitions-None-parititonFunc-…" class="headerlink" title="reduceByKey(func, numPartitions = None, parititonFunc = …)"></a>reduceByKey(func, numPartitions = None, parititonFunc = …)</h4><p>使用associative and commutative reduce function融合值。就是按key对值做reduce。<br>与groupByKey不同，会进行一个类似mapreduce中的combine操作。<br>如果未指定numPartitions，则输出将使用numPartition分区进行分区，或默认并行级别。 默认分区器是哈希分区。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from operator import add</span><br><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])</span><br><span class="line">&gt;&gt;&gt; sorted(rdd.reduceByKey(add).collect())</span><br><span class="line">[(&apos;a&apos;, 2), (&apos;b&apos;, 1)]</span><br></pre></td></tr></table></figure></p>
<p>reduceBykey 操作产生一个新的 RDD，其中 key 所有相同的的值组合成为一个 tuple - key 以及与 key 相关联的所有值在 reduce 函数上的执行结果。面临的挑战是，一个 key 的所有值不一定都在一个同一个 paritition 分区里，甚至是不一定在同一台机器里，但是它们必须共同被计算。</p>
<p>在 spark 里，特定的操作需要数据不跨分区分布。在计算期间，一个任务在一个分区上执行，为了所有数据都在单个 reduceByKey 的 reduce 任务上运行，我们需要执行一个 all-to-all 操作。它必须从所有分区读取所有的 key 和 key对应的所有的值，并且跨分区聚集去计算每个 key 的结果 - 这个过程就叫做 shuffle。</p>
<p>尽管每个分区新 shuffle 的数据集将是确定的，分区本身的顺序也是这样，但是这些数据的顺序是不确定的。如果希望 shuffle 后的数据是有序的，可以使用:</p>
<ul>
<li>mapPartitions 对每个 partition 分区进行排序，例如, sorted()</li>
<li>repartitionAndSortWithinPartitions 在分区的同时对分区进行高效的排序.</li>
<li>sortBy 对 RDD 进行全局的排序</li>
</ul>
<h4 id="reduceByKeyLocally-func"><a href="#reduceByKeyLocally-func" class="headerlink" title="reduceByKeyLocally(func)"></a>reduceByKeyLocally(func)</h4><p>合并key当中的values用一个关联的reduce函数操作。在 (K, V) pairs 的 dataset 上调用时, 返回 dataset of (K, V) pairs 的 dataset, 其中的 values 是针对每个 key 使用给定的函数 func 来进行聚合的, 它必须是 type (V,V) =&gt; V 的类型. 像 groupByKey 一样, reduce tasks 的数量是可以通过第二个可选的参数来配置的。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from operator import add</span><br><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])</span><br><span class="line">&gt;&gt;&gt; sorted(rdd.reduceByKeyLocally(add).items())</span><br><span class="line">[(&apos;a&apos;, 2), (&apos;b&apos;, 1)]</span><br></pre></td></tr></table></figure></p>
<h4 id="treeReduce-f-depth-2"><a href="#treeReduce-f-depth-2" class="headerlink" title="treeReduce(f, depth=2)"></a>treeReduce(f, depth=2)</h4><p>以 multi-level tree 形式 reduce RDD 中的元素。<br>参数：<br>depth ：建议树的深度（默认值：2）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; add = lambda x, y: x + y</span><br><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)</span><br><span class="line">&gt;&gt;&gt; rdd.treeReduce(add)</span><br><span class="line">-5</span><br><span class="line">&gt;&gt;&gt; rdd.treeReduce(add, 1)</span><br><span class="line">-5</span><br><span class="line">&gt;&gt;&gt; rdd.treeReduce(add, 2)</span><br><span class="line">-5</span><br><span class="line">&gt;&gt;&gt; rdd.treeReduce(add, 5)</span><br><span class="line">-5</span><br><span class="line">&gt;&gt;&gt; rdd.treeReduce(add, 10)</span><br><span class="line">-5</span><br></pre></td></tr></table></figure>
<h4 id="lookup-key"><a href="#lookup-key" class="headerlink" title="lookup(key)"></a>lookup(key)</h4><p>返回RDD中 key 键对应的值列表。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; l = range(1000)</span><br><span class="line">&gt;&gt;&gt; rdd = sc.parallelize(zip(l, l), 10)</span><br><span class="line">&gt;&gt;&gt; rdd.lookup(42)  # slow</span><br><span class="line">[42]</span><br><span class="line">&gt;&gt;&gt; sorted = rdd.sortByKey()</span><br><span class="line">&gt;&gt;&gt; sorted.lookup(42)  # fast</span><br><span class="line">[42]</span><br><span class="line">&gt;&gt;&gt; sorted.lookup(1024)</span><br><span class="line">[]</span><br><span class="line">&gt;&gt;&gt; rdd2 = sc.parallelize([((&apos;a&apos;, &apos;b&apos;), &apos;c&apos;)]).groupByKey()</span><br><span class="line">&gt;&gt;&gt; list(rdd2.lookup((&apos;a&apos;, &apos;b&apos;))[0])</span><br><span class="line">[&apos;c&apos;]</span><br></pre></td></tr></table></figure></p>
<h4 id="first"><a href="#first" class="headerlink" title="first()"></a>first()</h4><p>action函数。<br>返回RDD中的第一个元素，相当于 <code>take(1)</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([2, 3, 4]).first()</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([]).first()</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">    ...</span><br><span class="line">ValueError: RDD is empty</span><br></pre></td></tr></table></figure></p>
<h4 id="top-num-key-None"><a href="#top-num-key-None" class="headerlink" title="top(num, key = None)"></a>top(num, key = None)</h4><p>得到RDD中最大的 N 个元素。<em>注意</em>：返回到内存中。<br><em>注意</em>：它返回按降序排列的列表。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([10, 4, 2, 12, 3]).top(1)</span><br><span class="line">[12]</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([2, 3, 4, 5, 6], 2).top(2)</span><br><span class="line">[6, 5]</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)</span><br><span class="line">[4, 3, 2]</span><br></pre></td></tr></table></figure></p>
<h4 id="max-key-None"><a href="#max-key-None" class="headerlink" title="max(key = None)"></a>max(key = None)</h4><p>找到此RDD中的最大项目。</p>
<p>参数：key - 用于生成用于比较的键的函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])</span><br><span class="line">&gt;&gt;&gt; rdd.max()</span><br><span class="line">43.0</span><br><span class="line">&gt;&gt;&gt; rdd.max(key=str)</span><br><span class="line">5.0</span><br></pre></td></tr></table></figure>
<h4 id="min-key-None"><a href="#min-key-None" class="headerlink" title="min(key = None)"></a>min(key = None)</h4><p>与max类似。找到最小项</p>
<h4 id="take-num"><a href="#take-num" class="headerlink" title="take(num)"></a>take(num)</h4><p>action函数<br>将RDD 中前 num 个元素作为一个数组，返回它首先扫描一个分区，并使用该分区的结果来估计满足限制所需的其他分区数。<br><em>注意</em>：只有当生成的数组预期为小时，才应使用此方法，因为所有数据都将加载到驱动程序的内存中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)</span><br><span class="line">[2, 3]</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([2, 3, 4, 5, 6]).take(10)</span><br><span class="line">[2, 3, 4, 5, 6]</span><br><span class="line">&gt;&gt;&gt; sc.parallelize(range(100), 100).filter(lambda x: x &gt; 90).take(3)</span><br><span class="line">[91, 92, 93]</span><br></pre></td></tr></table></figure></p>
<h4 id="takeOrdered-num-key-None"><a href="#takeOrdered-num-key-None" class="headerlink" title="takeOrdered(num, key = None)"></a>takeOrdered(num, key = None)</h4><p>action函数。<br>返回 RDD 按自然顺序（natural order）或自定义比较器（custom comparator）排序后的前 n 个元素.同样会加载到内存中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)</span><br><span class="line">[1, 2, 3, 4, 5, 6]</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)</span><br><span class="line">[10, 9, 7, 6, 5, 4]</span><br></pre></td></tr></table></figure></p>
<h4 id="takeSample-withReplacement-num-seed-None"><a href="#takeSample-withReplacement-num-seed-None" class="headerlink" title="takeSample(withReplacement, num, seed=None)"></a>takeSample(withReplacement, num, seed=None)</h4><p>action函数。<br>对一个 dataset 进行随机抽样，返回一个包含 num 个随机抽样（random sample）元素的数组，参数 withReplacement 指定是否有放回抽样，参数 seed 指定生成随机数的种子。同样会加载到内存中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize(range(0, 10))</span><br><span class="line">&gt;&gt;&gt; len(rdd.takeSample(True, 20, 1))</span><br><span class="line">20</span><br><span class="line">&gt;&gt;&gt; len(rdd.takeSample(False, 5, 2))</span><br><span class="line">5</span><br><span class="line">&gt;&gt;&gt; len(rdd.takeSample(False, 15, 3))</span><br><span class="line">10</span><br></pre></td></tr></table></figure></p>
<h4 id="toLocalIterator"><a href="#toLocalIterator" class="headerlink" title="toLocalIterator()"></a>toLocalIterator()</h4><p>返回一个包含此RDD中所有元素的迭代器。 迭代器将消耗与RDD中最大分区一样多的内存。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize(range(10))</span><br><span class="line">&gt;&gt;&gt; [x for x in rdd.toLocalIterator()]</span><br><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p>
<h4 id="id"><a href="#id" class="headerlink" title="id()"></a>id()</h4><p>此RDD的唯一ID（在其SparkContext中）。</p>
<h4 id="setName-name"><a href="#setName-name" class="headerlink" title="setName(name)"></a>setName(name)</h4><p>为此RDD分配名称.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd1 = sc.parallelize([1, 2])</span><br><span class="line">&gt;&gt;&gt; rdd1.setName(&apos;RDD1&apos;).name()</span><br><span class="line">u&apos;RDD1&apos;</span><br></pre></td></tr></table></figure></p>
<h4 id="getNumPartitions"><a href="#getNumPartitions" class="headerlink" title="getNumPartitions()"></a>getNumPartitions()</h4><p>返回RDD中的分区数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 2)</span><br><span class="line">&gt;&gt;&gt; rdd.getNumPartitions()</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<h4 id="name"><a href="#name" class="headerlink" title="name()"></a>name()</h4><p>返回RDD名字</p>
<h4 id="isEmpty"><a href="#isEmpty" class="headerlink" title="isEmpty()"></a>isEmpty()</h4><p>当且仅当RDD不包含任何元素时才返回true。</p>
<blockquote>
<p>注意，即使RDD至少有1个分区，RDD也可能为空。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([]).isEmpty()</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([1]).isEmpty()</span><br><span class="line">False</span><br></pre></td></tr></table></figure>
<h4 id="外部数据集"><a href="#外部数据集" class="headerlink" title="外部数据集"></a>外部数据集</h4><p>文本文件RDD可以使用SparkContext的<code>textFile</code>方法创建。 该方法为文件（机器上的本地路径或hdfs：//、s3n：//等等）获取URI，并将其作为行的集合读取。 这是一个示例调用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distFile = sc.textFile(&quot;data.txt&quot;)</span><br></pre></td></tr></table></figure></p>
<p>一旦创建，distFile可以通过数据集操作来执行。例如，我们可以使用<code>map</code>和<code>reduce</code>操作来加出总所有行的大小，如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)</span><br></pre></td></tr></table></figure></p>
<p><code>SparkContext.wholeTextFiles</code>允许您读取包含多个小文本文件的目录，并将它们作为（文件名，内容）对返回。这与textFile相反，textFile将把每个文件的每行作为一个记录返回。</p>
<h5 id="saveAsHadoopDataset-conf-keyConverter-none-valueConverter-None"><a href="#saveAsHadoopDataset-conf-keyConverter-none-valueConverter-None" class="headerlink" title="saveAsHadoopDataset(conf, keyConverter = none, valueConverter = None)"></a>saveAsHadoopDataset(conf, keyConverter = none, valueConverter = None)</h5><p>使用旧的Hadoop OutputFormat API（映射包）将任意一个键值对的RDD（RDD [（K，V）]）输出到任何Hadoop文件系统。 键/值将使用用户指定的转换器转换为输出，或者默认。<br>。。。</p>
<h5 id="saveAsNewAPIHadoopDataset-conf-keyConverter-None-valueConverter-None"><a href="#saveAsNewAPIHadoopDataset-conf-keyConverter-None-valueConverter-None" class="headerlink" title="saveAsNewAPIHadoopDataset(conf, keyConverter=None, valueConverter=None)"></a>saveAsNewAPIHadoopDataset(conf, keyConverter=None, valueConverter=None)</h5><h5 id="saveAsHadoopFile-…"><a href="#saveAsHadoopFile-…" class="headerlink" title="saveAsHadoopFile(…)"></a>saveAsHadoopFile(…)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saveAsHadoopFile(path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)</span><br></pre></td></tr></table></figure>
<h5 id="saveAsNewAPIHadoopFile-…"><a href="#saveAsNewAPIHadoopFile-…" class="headerlink" title="saveAsNewAPIHadoopFile(…)"></a>saveAsNewAPIHadoopFile(…)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saveAsNewAPIHadoopFile(path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)</span><br></pre></td></tr></table></figure>
<h5 id="saveAsPickleFile-path-batchSize-10"><a href="#saveAsPickleFile-path-batchSize-10" class="headerlink" title="saveAsPickleFile(path, batchSize=10)"></a>saveAsPickleFile(path, batchSize=10)</h5><p>将此RDD保存为序列化对象的SequenceFile。 使用的serializer是<code>pyspark.serializers.PickleSerializer</code>，默认批量大小为10。<br><code>RDD.saveAsPickleFile</code>和<code>SparkContext.pickleFile</code>支持以包含Pickle Python对象的简单格式保存RDD。批次用于酸洗序列化，默认批量大小为10。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#tempfile库，创建一个有名字的临时文件</span><br><span class="line">&gt;&gt;&gt; tmpFile = NamedTemporaryFile(delete=True)</span><br><span class="line">&gt;&gt;&gt; tmpFile.close()</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([1, 2, &apos;spark&apos;, &apos;rdd&apos;]).saveAsPickleFile(tmpFile.name, 3)</span><br><span class="line">&gt;&gt;&gt; sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())</span><br><span class="line">[&apos;1&apos;, &apos;2&apos;, &apos;rdd&apos;, &apos;spark&apos;]</span><br></pre></td></tr></table></figure>
<h5 id="saveAsSequenceFile-path-compressionCodecClass-None"><a href="#saveAsSequenceFile-path-compressionCodecClass-None" class="headerlink" title="saveAsSequenceFile(path, compressionCodecClass=None)"></a>saveAsSequenceFile(path, compressionCodecClass=None)</h5><p>使用从RDD的键和值类型转换的org.apache.hadoop.io.Writable类型，将任何键值对的RDD（RDD [（K，V）]）输出到任何Hadoop文件系统。<br>将 dataset 中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统指定的路径中。该操作可以在实现了 Hadoop 的 Writable 接口的键值对（key-value pairs）的 RDD 上使用。在 Scala 中，它还可以隐式转换为 Writable 的类型（Spark 包括了基本类型的转换，例如 Int, Double, String 等等).</p>
<h5 id="saveAsTextFile-path-compressionCodecClass-None"><a href="#saveAsTextFile-path-compressionCodecClass-None" class="headerlink" title="saveAsTextFile(path, compressionCodecClass=None)"></a>saveAsTextFile(path, compressionCodecClass=None)</h5><p>action函数<br>将 dataset 中的元素以文本文件（或文本文件集合）的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中的给定目录中。Spark 将对每个元素调用 toString 方法，将数据元素转换为文本文件中的一行记录（即使用元素的字符串表示形式）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; tempFile = NamedTemporaryFile(delete=True)</span><br><span class="line">&gt;&gt;&gt; tempFile.close()</span><br><span class="line">&gt;&gt;&gt; sc.parallelize(range(10)).saveAsTextFile(tempFile.name)</span><br><span class="line">&gt;&gt;&gt; from fileinput import input</span><br><span class="line">&gt;&gt;&gt; from glob import glob</span><br><span class="line">&gt;&gt;&gt; &apos;&apos;.join(sorted(input(glob(tempFile.name + &quot;/part-0000*&quot;))))</span><br><span class="line">&apos;0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n&apos;</span><br></pre></td></tr></table></figure>
<p>保存为文本文件时，可以容忍空行。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; tempFile2 = NamedTemporaryFile(delete=True)</span><br><span class="line">&gt;&gt;&gt; tempFile2.close()</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([&apos;&apos;, &apos;foo&apos;, &apos;&apos;, &apos;bar&apos;, &apos;&apos;]).saveAsTextFile(tempFile2.name)</span><br><span class="line">&gt;&gt;&gt; &apos;&apos;.join(sorted(input(glob(tempFile2.name + &quot;/part-0000*&quot;))))</span><br><span class="line">&apos;\n\n\nbar\nfoo\n&apos;</span><br></pre></td></tr></table></figure></p>
<h4 id="外部传递函数给spark"><a href="#外部传递函数给spark" class="headerlink" title="外部传递函数给spark"></a>外部传递函数给spark</h4><p>Spark的API很大程度上依赖于驱动程序中的传递函数来在集群上运行。 推荐的方法有三种：</p>
<ol>
<li><p>Lambda表达式，用于可以写为表达式的简单函数。 （Lambdas不支持多语句函数或不返回值的语句。）</p>
</li>
<li><p>本地def内部的函数调用到Spark，更长的代码。</p>
</li>
<li><p>模块中的Top-level functions。</p>
</li>
</ol>
<p>例如，要传递比使用lambda支持的更长的函数，请考虑以下代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;MyScript.py&quot;&quot;&quot;</span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    def myFunc(s):</span><br><span class="line">        words = s.split(&quot; &quot;)</span><br><span class="line">        return len(words)</span><br><span class="line"></span><br><span class="line">    sc = SparkContext(...)</span><br><span class="line">    sc.textFile(&quot;file.txt&quot;).map(myFunc)</span><br></pre></td></tr></table></figure></p>
<p>请注意，虽然也可以传递对类实例（而不是单例对象）中的方法的引用，但这需要发送包含该类的对象以及该方法。 例如，考虑：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class MyClass(object):</span><br><span class="line">    def func(self, s):</span><br><span class="line">        return s</span><br><span class="line">    def doStuff(self, rdd):</span><br><span class="line">        return rdd.map(self.func)</span><br></pre></td></tr></table></figure></p>
<p>在这里，如果我们创建一个新的MyClass并在其上调用doStuff，那么里面的map会引用MyClass实例的func方法，所以整个对象需要发送到集群。</p>
<p>以类似的方式，访问外部对象的字段将引用整个对象：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class MyClass(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.field = &quot;Hello&quot;</span><br><span class="line">    def doStuff(self, rdd):</span><br><span class="line">        return rdd.map(lambda s: self.field + s)</span><br></pre></td></tr></table></figure></p>
<p>为了避免这个问题，最简单的方法是将字段复制到局部变量中，而不是外部访问它。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def doStuff(self, rdd):</span><br><span class="line">    field = self.field</span><br><span class="line">    return rdd.map(lambda s: field + s)</span><br></pre></td></tr></table></figure></p>
<h4 id="理解闭包"><a href="#理解闭包" class="headerlink" title="理解闭包"></a>理解闭包</h4><p>在集群中执行代码时，一个关于 Spark 更难的事情是理解变量和方法的范围和生命周期。<br>不能像以下这样写：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">counter = 0</span><br><span class="line">rdd = sc.parallelize(data)</span><br><span class="line"></span><br><span class="line"># Wrong: Don&apos;t do this!!</span><br><span class="line">def increment_counter(x):</span><br><span class="line">    global counter</span><br><span class="line">    counter += x</span><br><span class="line">rdd.foreach(increment_counter)</span><br><span class="line"></span><br><span class="line">print(&quot;Counter value: &quot;, counter)</span><br></pre></td></tr></table></figure></p>
<p>上面的代码行为是不确定的，并且可能无法按预期正常工作。执行作业时，Spark 会分解 RDD 操作到每个 executor 中的 task 里。在执行之前，Spark 计算任务的 closure（闭包）。而闭包是在 RDD 上的 executor 必须能够访问的变量和方法（在此情况下的 foreach()）。闭包被序列化并被发送到每个执行器。</p>
<p>闭包的变量副本发给每个 counter ，当 counter 被 foreach 函数引用的时候，它已经不再是 driver node 的 counter 了。虽然在 driver node 仍然有一个 counter 在内存中，但是对 executors 已经不可见。executor 看到的只是序列化的闭包一个副本。所以 counter 最终的值还是 0，因为对 counter 所有的操作均引用序列化的 closure 内的值。</p>
<p><strong>应该使用累加器</strong></p>
<p>类似地，另一种常见的语法用于打印 RDD 的所有元素使用<code>rdd.foreach(println)</code>或 <code>rdd.map(println)</code>。在一台机器上，这将产生预期的输出和打印 RDD 的所有元素。然而，在集群 cluster 模式下，stdout 输出正在被执行写操作 executors 的 stdout 代替，而不是在一个驱动程序上，因此 stdout 的 driver 程序不会显示这些！要打印 driver 程序的所有元素，可以使用的 <code>collect()</code> 方法首先把 RDD 放到 driver 程序节点上: <code>rdd.collect().foreach(println)</code>。这可能会导致 driver 程序耗尽内存，虽说，因为 <code>collect()</code> 获取整个 RDD 到一台机器; 如果你只需要打印 RDD 的几个元素，一个更安全的方法是使用 <code>take()</code>: <code>rdd.take(100).foreach(println)</code>。</p>
<h4 id="键-值对"><a href="#键-值对" class="headerlink" title="键-值对"></a>键-值对</h4><p>虽然大多数Spark操作适用于包含任何类型对象的RDD，但是几个特殊操作只能在键值对的RDD上使用。 最常见的是分布式“shuffle”操作，例如按键分组或聚合元素。</p>
<p>在Python中，这些操作适用于包含内置Python元组的RDD，例如(1, 2)。 只需创建这样的元组，然后调用所需的操作。</p>
<p>例如，以下代码使用键值对上的<code>reduceByKey</code>操作来计算每行文本在文件中的发生次数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lines = sc.textFile(&quot;data.txt&quot;)</span><br><span class="line">pairs = lines.map(lambda s: (s, 1))</span><br><span class="line">counts = pairs.reduceByKey(lambda a, b: a + b)</span><br></pre></td></tr></table></figure>
<p>例如，我们也可以使用<code>counts.sortByKey()</code>来按字母顺序排列对，最后将<code>counts.collect()</code>作为对象的列表返回到驱动程序。</p>
<h4 id="Shuffle操作"><a href="#Shuffle操作" class="headerlink" title="Shuffle操作"></a>Shuffle操作</h4><p>Spark 里的某些操作会触发 shuffle。shuffle 是spark 重新分配数据的一种机制，使得这些数据可以跨不同的区域进行分组。这通常涉及在 executors 和 机器之间拷贝数据，这使得 shuffle 成为一个复杂的、代价高的操作。</p>
<p>触发的 shuffle 操作包括 <code>repartition</code> 操作，如 <code>repartition</code> 和 <code>coalesce</code>, <code>‘ByKey</code> 操作 (除了 counting 之外) 像 <code>groupByKey</code> 和 <code>reduceByKey</code>, 和 <code>join</code> 操作, 像 <code>cogroup</code> 和 <code>join</code>。</p>
<h5 id="性能影响"><a href="#性能影响" class="headerlink" title="性能影响"></a>性能影响</h5><p>该 Shuffle 是一个代价比较高的操作，它涉及磁盘 I/O、数据序列化、网络 I/O。为了准备 shuffle 操作的数据，Spark 启动了一系列的任务，map 任务组织数据，reduce 完成数据的聚合。这些术语来自 MapReduce，跟 Spark 的 map 操作和 reduce 操作没有关系。</p>
<p>在内部，一个 map 任务的所有结果数据会保存在内存，直到内存不能全部存储为止。然后，这些数据将基于目标分区进行排序并写入一个单独的文件中。在 reduce 时，任务将读取相关的已排序的数据块。</p>
<p>某些 shuffle 操作会大量消耗堆内存空间，因为 shuffle 操作在数据转换前后，需要在使用内存中的数据结构对数据进行组织。需要特别说明的是，reduceByKey 和 aggregateByKey 在 map 时会创建这些数据结构，’ByKey 操作在 reduce 时创建这些数据结构。当内存满的时候，Spark 会把溢出的数据存到磁盘上，这将导致额外的磁盘 I/O 开销和垃圾回收开销的增加。</p>
<p>shuffle 操作还会在磁盘上生成大量的中间文件。在 Spark 1.3 中，这些文件将会保留至对应的 RDD 不在使用并被垃圾回收为止。这么做的好处是，如果在 Spark 重新计算 RDD 的血统关系（lineage）时，shuffle 操作产生的这些中间文件不需要重新创建。如果 Spark 应用长期保持对 RDD 的引用，或者垃圾回收不频繁，这将导致垃圾回收的周期比较长。这意味着，长期运行 Spark 任务可能会消耗大量的磁盘空间。临时数据存储路径可以通过 SparkContext 中设置参数 spark.local.dir 进行配置。</p>
<h4 id="RDD-Persistence（持久化）"><a href="#RDD-Persistence（持久化）" class="headerlink" title="RDD Persistence（持久化）"></a>RDD Persistence（持久化）</h4><p>Spark 中一个很重要的能力是将数据 persisting 持久化（或称为 caching 缓存），在多个操作间都可以访问这些持久化的数据。当持久化一个 RDD 时，每个节点的其它分区都可以使用 RDD 在内存中进行计算，在该数据上的其他 action 操作将直接使用内存中的数据。这样会让以后的 action 操作计算速度加快（通常运行速度会加速 10 倍）。缓存是迭代算法和快速的交互式使用的重要工具。</p>
<p>RDD 可以使用 <code>persist()</code> 方法或 <code>cache()</code> 方法进行持久化。数据将会在第一次 action 操作时进行计算，并缓存在节点的内存中。Spark 的缓存具有容错机制，如果一个缓存的 RDD 的某个分区丢失了，Spark 将按照原来的计算过程，自动重新计算并进行缓存。</p>
<p>另外，每个持久化的 RDD 可以使用不同的 storage level 存储级别进行缓存，例如，持久化到磁盘、已序列化的 Java 对象形式持久化到内存（可以节省空间）、跨节点间复制、以 off-heap 的方式存储在 Tachyon。这些存储级别通过传递一个 StorageLevel 对象 (Scala, Java, Python) 给 persist() 方法进行设置。<code>cache()</code> 方法是使用默认存储级别的快捷设置方法，默认的存储级别是 StorageLevel.MEMORY_ONLY（将反序列化的对象存储到内存中）。</p>
<p>存储级别还是看<a href="http://spark.apachecn.org/docs/cn/2.2.0/rdd-programming-guide.html" target="_blank" rel="noopener">文档</a></p>
<p>在 Python 中, stored objects will 总是使用 Pickle library 来序列化对象, 所以无论你选择序列化级别都没关系. 在 Python 中可用的存储级别有 MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2, DISK_ONLY, 和 DISK_ONLY_2.</p>
<p>在 shuffle 操作中（例如 reduceByKey），即便是用户没有调用 <code>persist</code> 方法，Spark 也会自动缓存部分中间数据.这么做的目的是，在 shuffle 的过程中某个节点运行失败时，不需要重新计算所有的输入数据。如果用户想多次使用某个 RDD，强烈推荐在该 RDD 上调用 <code>persist()</code> 方法。</p>
<h5 id="如何选择存储级别"><a href="#如何选择存储级别" class="headerlink" title="如何选择存储级别"></a>如何选择存储级别</h5><p>Spark 的存储级别的选择，核心问题是在 memory 内存使用率和 CPU 效率之间进行权衡。建议按下面的过程进行存储级别的选择：<br>如果您的 RDD 适合于默认存储级别 (MEMORY_ONLY), leave them that way. 这是CPU效率最高的选项，允许RDD上的操作尽可能快地运行.</p>
<p>如果不是, 试着使用 MEMORY_ONLY_SER 和 selecting a fast serialization library 以使对象更加节省空间，但仍然能够快速访问。 (Java和Scala)</p>
<p>不要溢出到磁盘，除非计算您的数据集的函数是昂贵的, 或者它们过滤大量的数据. 否则, 重新计算分区可能与从磁盘读取分区一样快.</p>
<p>如果需要快速故障恢复，请使用复制的存储级别 (e.g. 如果使用Spark来服务 来自网络应用程序的请求). All 存储级别通过重新计算丢失的数据来提供完整的容错能力，但复制的数据可让您继续在 RDD 上运行任务，而无需等待重新计算一个丢失的分区.</p>
<h5 id="删除数据"><a href="#删除数据" class="headerlink" title="删除数据"></a>删除数据</h5><p>Spark 会自动监视每个节点上的缓存使用情况，并使用 least-recently-used（LRU）的方式来丢弃旧数据分区。 如果您想手动删除 RDD 而不是等待它掉出缓存，使用 <code>RDD.unpersist()</code> 方法。</p>
<h3 id="部署应用到集群中"><a href="#部署应用到集群中" class="headerlink" title="部署应用到集群中"></a>部署应用到集群中</h3><p>该 <a href="http://spark.apachecn.org/docs/cn/2.2.0/submitting-applications.html" target="_blank" rel="noopener">应用提交指南</a> 描述了如何将应用提交到集群中. 简单的说, 在您将应用打包成一个JAR(针对 Java/Scala) 或者一组 .py 或 .zip 文件 (针对Python), 该 bin/spark-submit 脚本可以让你提交它到任何所支持的 cluster manager 上去.</p>
<h3 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h3><p>Spark 可以友好的使用流行的单元测试框架进行单元测试。在将 master URL 设置为 local 来测试时会简单的创建一个 SparkContext，运行您的操作，然后调用 <code>SparkContext.stop()</code> 将该作业停止。因为 Spark 不支持在同一个程序中并行的运行两个 <code>contexts</code>，所以需要确保使用 <code>finally</code> 块或者测试框架的<code>tearDown</code>方法将 <code>context</code> 停止。</p>
<h3 id="例程"><a href="#例程" class="headerlink" title="例程"></a>例程</h3><p><a href="https://github.com/apache/spark/tree/master/examples/src/main/python" target="_blank" rel="noopener">Spark程序示例</a></p>
<p>您可以通过传递 class name 到 Spark 的 spark-submit 脚本以运行 Python 示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit examples/src/main/python/pi.py</span><br></pre></td></tr></table></figure></p>
<h3 id="pyspark-sql-module"><a href="#pyspark-sql-module" class="headerlink" title="pyspark.sql module"></a>pyspark.sql module</h3><p>Spark SQL 和 DataFrames里的重要类</p>
<p>pyspark.sql.SparkSession –DataFrame 和 SQL 功能的主要进入点</p>
<p>pyspark.sql.DataFrame –以列命名的分布式数据集合</p>
<p>pyspark.sql.Column –DataFrame中以列表示的数据，也就是一列数据</p>
<p>pyspark.sql.Row –DataFrame中以行表示的数据，也就是一行数据</p>
<p>pyspark.sql.HiveContext –对Apache Hive中存储数据进行访问的主要进入点</p>
<p>pyspark.sql.GroupedData–DataFrame.groupBy() 返回的聚合方法</p>
<p>pyspark.sql.DataFrameNaFunctions –处理丢失数据的方法(null values).</p>
<p>pyspark.sql.DataFrameStatFunctions –统计功能的方法</p>
<p>pyspark.sql.functions  –DataFrame可用的内置函数列表</p>
<p>pyspark.sql.types –可用的数据类型列表。</p>
<p>pyspark.sql.Window –处理窗口功能（E）</p>
<h4 id="Dataset：Spark新的抽象层"><a href="#Dataset：Spark新的抽象层" class="headerlink" title="Dataset：Spark新的抽象层"></a>Dataset：Spark新的抽象层</h4><p>Spark最原始的抽象基础是RDD（分布式弹性数据集），但是从Spark 2.0 开始，Dataset将成为Spark新的抽象层。所有的Spark开发者将使用Dataset API和Dataframe（Dataset子集）API编写代码，同时RDD API也还是可以用的，不过已降为low-level的API。</p>
<p>Dataframe API 在Spark 1.3时被引入，Dataset是Dataframe的超集。Dataset API和Dataframe API的使用带给Spark更好的性能和灵活性。Spark Streaming也将使用Dataset代替RDD。</p>
<h4 id="Spark-Session：Spark-2-0入口"><a href="#Spark-Session：Spark-2-0入口" class="headerlink" title="Spark Session：Spark 2.0入口"></a>Spark Session：Spark 2.0入口</h4><p>在Spark早期版本，spark context是Spark的入口，RDD API通过context API创建。相应地，streaming由StreamingContext创建；SQL由sqlContext创建；hive由HiveContext创建。而到了Spark 2.0，DataSet和Dataframe API由Spark Session创建。</p>
<p>SparkSession包括SQLContext，HiveContext和StreamingContext的功能。Spark session实际起计算的还是Spark context。</p>
<h4 id="class-pyspark-sql-SparkSession"><a href="#class-pyspark-sql-SparkSession" class="headerlink" title="class pyspark.sql.SparkSession"></a>class pyspark.sql.SparkSession</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class pyspark.sql.SparkSession(sparkContext, jsparkSession=None)</span><br></pre></td></tr></table></figure>
<p>使用Dataset和DataFrame API编程Spark的入门点。</p>
<p>可以使用<code>SparkSession</code>创建DataFrame，将DataFrame注册为表，在表上执行SQL，缓存表和读取拼贴文件。<br>要创建<code>SparkSession</code>，请使用以下构建器模式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">spark = SparkSession\</span><br><span class="line">      .builder\</span><br><span class="line">      .master(&quot;local&quot;) \</span><br><span class="line">      .appName(&quot;PythonWordCount&quot;)\</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \</span><br><span class="line">      .getOrCreate()</span><br></pre></td></tr></table></figure></p>
<h5 id="class-pyspark-sql-HiveContext-sparkContext-jhiveContext-None"><a href="#class-pyspark-sql-HiveContext-sparkContext-jhiveContext-None" class="headerlink" title="class pyspark.sql.HiveContext(sparkContext, jhiveContext=None)"></a>class pyspark.sql.HiveContext(sparkContext, jhiveContext=None)</h5><p>Spark SQL的一个变体，与存储在Hive中的数据集成在一起。</p>
<p>Hive的配置从类路径中的hive-site.xml读取。 它支持运行SQL和HiveQL命令。</p>
<p>注意在2.0.0中已弃用。 使用<code>SparkSession.builder.enableHiveSupport().getOrCreate()</code>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val sparkSession = SparkSession.builder.</span><br><span class="line">      master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;spark session example&quot;)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .getOrCreate()</span><br></pre></td></tr></table></figure></p>
<h5 id="读数据"><a href="#读数据" class="headerlink" title="读数据"></a>读数据</h5><p>使用Spark Session读取CSV数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val df = sparkSession.read.option(&quot;header&quot;,&quot;true&quot;).</span><br><span class="line">    csv(&quot;src/main/resources/sales.csv&quot;)</span><br></pre></td></tr></table></figure>
<h5 id="buider"><a href="#buider" class="headerlink" title="buider"></a>buider</h5><p>SparkSession的构建器。</p>
<ul>
<li><p>appName(name)<br>设置应用程序的名称，这将在Spark Web UI中显示。如果没有设置应用程序名称，将使用随机生成的名称。</p>
</li>
<li><p>config（key = None，value = None，conf = None）<br>设置一个配置选项。 使用此方法设置的选项将自动传播到SparkConf和SparkSession自己的配置。对于现有的SparkConf，使用<code>conf</code>参数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from pyspark.conf import SparkConf</span><br><span class="line">&gt;&gt;&gt; SparkSession.builder.config(conf=SparkConf())</span><br><span class="line">&lt;pyspark.sql.session...</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>键值对可以省略参数名称<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; SparkSession.builder.config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">&lt;pyspark.sql.session...</span><br></pre></td></tr></table></figure></p>
<p>参数：<br>key–配置属性的键名字符串<br>value–配置属性的值<br>conf–SparkConf的一个实例</p>
<ul>
<li><p>enableHiveSupport()<br>支持Hive支持，包括连接到持续的Hive转移，支持Hive serdes和Hive用户定义的功能。</p>
</li>
<li><p>getOrCreate()<br>获取现有的SparkSession，如果没有现有的SparkSession，则根据此构建器中设置的选项创建一个新的SparkSession。<br>此方法首先检查是否有有效的全局默认SparkSession，如果是，则返回一个。 如果没有有效的全局默认SparkSession存在，该方法将创建一个新的SparkSession并将新创建的SparkSession分配为全局默认值。</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s1 = SparkSession.builder.config(&quot;k1&quot;, &quot;v1&quot;).getOrCreate()</span><br><span class="line">&gt;&gt;&gt; s1.conf.get(&quot;k1&quot;) == s1.sparkContext.getConf().get(&quot;k1&quot;) == &quot;v1&quot;</span><br><span class="line">True</span><br></pre></td></tr></table></figure>
<p>如果返回现有的SparkSession，则此构建器中指定的配置选项将应用于现有的SparkSession。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s2 = SparkSession.builder.config(&quot;k2&quot;, &quot;v2&quot;).getOrCreate()</span><br><span class="line">&gt;&gt;&gt; s1.conf.get(&quot;k1&quot;) == s2.conf.get(&quot;k1&quot;)</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; s1.conf.get(&quot;k2&quot;) == s2.conf.get(&quot;k2&quot;)</span><br><span class="line">True</span><br></pre></td></tr></table></figure></p>
<ul>
<li>master(master)</li>
</ul>
<p>设置要连接的Spark主URL，例如“local”在本地运行，“local [4]”；以4个内核在本地运行，或“spark：// master：7077”在Spark独立群集上运行。<br>参数：<strong>master</strong>–a url for spark master</p>
<h5 id="SparkSession-catalog"><a href="#SparkSession-catalog" class="headerlink" title="SparkSession.catalog"></a>SparkSession.catalog</h5><p>用户可以创建，删除，更改或查询底层数据库，表格，功能等的接口。</p>
<h5 id="SparkSession-conf"><a href="#SparkSession-conf" class="headerlink" title="SparkSession.conf"></a>SparkSession.conf</h5><p>Spark的运行时配置接口。</p>
<p>这是用户可以获取和设置与Spark SQL相关的所有Spark和Hadoop配置的界面。 获取配置值时，默认值为底层SparkContext中设置的值（如果有）。</p>
<h5 id="SparkSession-createDataFrame"><a href="#SparkSession-createDataFrame" class="headerlink" title="SparkSession.createDataFrame"></a>SparkSession.createDataFrame</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SparkSession.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)</span><br></pre></td></tr></table></figure>
<p>根据一个RDD或 list ，或 pandas.DataFrame 创建一个DataFrame，</p>
<p>当schema是列名列表（a list of column names）时，每个列的类型将从数据中推断出来。</p>
<p>当schema为<code>None</code>时，它将尝试从数据中推断出schema（列名和类型），它们应该是Row，或者是namedtuple或dict的RDD。</p>
<p>当schema是<code>pyspark.sql.types.DataType</code>或数据类型字符串时，它必须与实际数据匹配，否则将在运行时抛出异常。 如果给定的schema不是<code>pyspark.sql.types.StructType</code>，它将被包装到一个<code>pyspark.sql.types.StructType</code>作为其唯一的字段，并且字段名称将是“值”，每个记录也将被包装成 元组，可以稍后转换成行。</p>
<p>如果需要schema inference，则使用samplingRatio来确定用于schema inference的行的比例。 如果samplingRatio为None，则会使用第一行。（模式推理是一种用于在解析任何XML文档的结构之后推断XSD（XML模式定义）的技术。）</p>
<p><strong>参数</strong> ：</p>
<ul>
<li>data – 任何类型的SQL数据或列表，或<code>pandas.DataFrame</code>表示的RDD（例如，row，tuple，int，boolean等）。</li>
<li>schema –<code>pyspark.sql.types.DataType</code>或数据类型字符串或列名称列表，默认值为<code>None</code>。 数据类型字符串格式等于<code>pyspark.sql.types.DataType.simpleString</code>，除了顶级结构类型可以省略<code>struct &lt;&gt;</code>和原子类型使用<code>typeName()</code>作为其格式，例如。 对于<code>pyspark.sql.types.ByteType</code>使用<code>byte</code>而不是<code>tinyint</code>。 我们也可以使用int作为IntegerType的简称。</li>
<li>samplingRatio –the sample ratio of rows used for inferring</li>
<li>verifySchema – verify data types of every row against schema.<br><strong>返回</strong>：DataFrame<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; l = [(&apos;Alice&apos;, 1)]</span><br><span class="line">&gt;&gt;&gt; spark.createDataFrame(l).collect()</span><br><span class="line">[Row(_1=u&apos;Alice&apos;, _2=1)]</span><br><span class="line">&gt;&gt;&gt; spark.createDataFrame(l, [&apos;name&apos;, &apos;age&apos;]).collect()</span><br><span class="line">[Row(name=u&apos;Alice&apos;, age=1)]</span><br><span class="line">======</span><br><span class="line">&gt;&gt;&gt; d = [&#123;&apos;name&apos;: &apos;Alice&apos;, &apos;age&apos;: 1&#125;]</span><br><span class="line">&gt;&gt;&gt; spark.createDataFrame(d).collect()</span><br><span class="line">[Row(age=1, name=u&apos;Alice&apos;)]</span><br><span class="line">======</span><br><span class="line">&gt;&gt;&gt; rdd = sc.parallelize(l)</span><br><span class="line">&gt;&gt;&gt; spark.createDataFrame(rdd).collect()</span><br><span class="line">[Row(_1=u&apos;Alice&apos;, _2=1)]</span><br><span class="line">&gt;&gt;&gt; df = spark.createDataFrame(rdd, [&apos;name&apos;, &apos;age&apos;])</span><br><span class="line">&gt;&gt;&gt; df.collect()</span><br><span class="line">[Row(name=u&apos;Alice&apos;, age=1)]</span><br><span class="line">======</span><br><span class="line">&gt;&gt;&gt; from pyspark.sql import Row</span><br><span class="line">&gt;&gt;&gt; Person = Row(&apos;name&apos;, &apos;age&apos;)</span><br><span class="line">&gt;&gt;&gt; person = rdd.map(lambda r: Person(*r))</span><br><span class="line">&gt;&gt;&gt; df2 = spark.createDataFrame(person)</span><br><span class="line">&gt;&gt;&gt; df2.collect()</span><br><span class="line">[Row(name=u&apos;Alice&apos;, age=1)]</span><br><span class="line">======</span><br><span class="line">&gt;&gt;&gt; from pyspark.sql import Row</span><br><span class="line">&gt;&gt;&gt; Person = Row(&apos;name&apos;, &apos;age&apos;)</span><br><span class="line">&gt;&gt;&gt; person = rdd.map(lambda r: Person(*r))</span><br><span class="line">&gt;&gt;&gt; df2 = spark.createDataFrame(person)</span><br><span class="line">&gt;&gt;&gt; df2.collect()</span><br><span class="line">[Row(name=u&apos;Alice&apos;, age=1)]</span><br><span class="line">======</span><br><span class="line">&gt;&gt;&gt; from pyspark.sql.types import *</span><br><span class="line">&gt;&gt;&gt; schema = StructType([</span><br><span class="line">...    StructField(&quot;name&quot;, StringType(), True),</span><br><span class="line">...    StructField(&quot;age&quot;, IntegerType(), True)])</span><br><span class="line">&gt;&gt;&gt; df3 = spark.createDataFrame(rdd, schema)</span><br><span class="line">&gt;&gt;&gt; df3.collect()</span><br><span class="line">[Row(name=u&apos;Alice&apos;, age=1)]</span><br><span class="line">======</span><br><span class="line">&gt;&gt;&gt; spark.createDataFrame(rdd, &quot;a: string, b: int&quot;).collect()</span><br><span class="line">[Row(a=u&apos;Alice&apos;, b=1)]</span><br><span class="line">&gt;&gt;&gt; rdd = rdd.map(lambda row: row[1])</span><br><span class="line">&gt;&gt;&gt; spark.createDataFrame(rdd, &quot;int&quot;).collect()</span><br><span class="line">[Row(value=1)]</span><br><span class="line">&gt;&gt;&gt; spark.createDataFrame(rdd, &quot;boolean&quot;).collect()</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">    ...</span><br><span class="line">Py4JJavaError: ...</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="SparkSession-newSession"><a href="#SparkSession-newSession" class="headerlink" title="SparkSession.newSession()"></a>SparkSession.newSession()</h5><p>将新的SparkSession返回为new session，具有单独的SQLConf，注册的临时视图和UDF，但共享SparkContext和表缓存。</p>
<h5 id="SparkSession-range-start-end-None-step-1-numPartitions-None"><a href="#SparkSession-range-start-end-None-step-1-numPartitions-None" class="headerlink" title="SparkSession.range(start, end=None, step=1, numPartitions=None)"></a>SparkSession.range(start, end=None, step=1, numPartitions=None)</h5><p>使用单个pyspark.sql.types.LongType列命名为id创建一个DataFrame，其中包含从<code>start</code>到<code>end</code>（独占）的范围内的元素，具有步长值<code>step</code>。</p>
<p><strong>返回</strong>：DataFrame<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; spark.range(1, 7, 2).collect()</span><br><span class="line">[Row(id=1), Row(id=3), Row(id=5)]</span><br></pre></td></tr></table></figure></p>
<h5 id="SparkSession-sparkContext"><a href="#SparkSession-sparkContext" class="headerlink" title="SparkSession.sparkContext"></a>SparkSession.sparkContext</h5><p>返回底层的SparkContext。</p>
<h5 id="SparkSession-read"><a href="#SparkSession-read" class="headerlink" title="SparkSession.read"></a>SparkSession.read</h5><p>返回一个DataFrameReader，可以用作DataFrame中的数据读取。</p>
<p>返回：DataFrameReader</p>
<h5 id="SparkSession-sql（sqlQuery）"><a href="#SparkSession-sql（sqlQuery）" class="headerlink" title="SparkSession.sql（sqlQuery）"></a>SparkSession.sql（sqlQuery）</h5><p>返回表示给定查询结果的DataFrame。</p>
<p><strong>返回</strong>：DataFrame</p>
<h6 id="SparkSession-stop"><a href="#SparkSession-stop" class="headerlink" title="SparkSession.stop()"></a>SparkSession.stop()</h6><p>Stop the underlying SparkContext.</p>
<h5 id="SparkSession-table-tableName"><a href="#SparkSession-table-tableName" class="headerlink" title="SparkSession.table(tableName)"></a>SparkSession.table(tableName)</h5><p>将指定的表作为DataFrame返回。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df.createOrReplaceTempView(&quot;table1&quot;)</span><br><span class="line">&gt;&gt;&gt; df2 = spark.table(&quot;table1&quot;)</span><br><span class="line">&gt;&gt;&gt; sorted(df.collect()) == sorted(df2.collect())</span><br><span class="line">True</span><br></pre></td></tr></table></figure></p>
<h4 id="class-pyspark-sql-SQLContext-sparkContext-sparkSession-None-jsqlContext-None"><a href="#class-pyspark-sql-SQLContext-sparkContext-sparkSession-None-jsqlContext-None" class="headerlink" title="class pyspark.sql.SQLContext(sparkContext, sparkSession=None, jsqlContext=None)"></a>class pyspark.sql.SQLContext(sparkContext, sparkSession=None, jsqlContext=None)</h4><p>在Spark 1.x中使用Spark中的结构化数据（行和列）的入口点。</p>
<p>从Spark 2.0开始，这被SparkSession所取代。 但是，我们正在为此向后兼容。</p>
<p>可以使用SQLContext创建DataFrame，将DataFrame注册为表，在表上执行SQL，缓存表和读取拼贴文件。</p>
<h4 id="class-pyspark-sql-HiveContext-sparkContext-jhiveContext-None-1"><a href="#class-pyspark-sql-HiveContext-sparkContext-jhiveContext-None-1" class="headerlink" title="class pyspark.sql.HiveContext(sparkContext, jhiveContext=None)"></a>class pyspark.sql.HiveContext(sparkContext, jhiveContext=None)</h4><p>Spark SQL的一个变体，与存储在Hive中的数据集成在一起。</p>
<p>Hive的配置从类路径中的hive-site.xml读取。 它支持运行SQL和HiveQL命令。</p>
<p>注意在2.0.0中已弃用。 使用<code>SparkSession.builder.enableHiveSupport().getOrCreate()</code>。</p>
<h5 id="refreshTable-tableName"><a href="#refreshTable-tableName" class="headerlink" title="refreshTable(tableName)"></a>refreshTable(tableName)</h5><p>无效并刷新所有缓存的给定表的元数据。 出于性能原因，Spark SQL或其使用的外部数据源库可能会缓存有关表的某些元数据，例如块的位置。 当这些更改在Spark SQL之外时，用户应该调用此函数来使缓存无效。</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/spark/">spark</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC8zNDUwMy8xMTA0MQ==">
<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
</script>
<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->
	</div>



      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
    
  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    
      <ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#主要结构介绍"><span class="nav-number">1.</span> <span class="nav-text">主要结构介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD"><span class="nav-number">1.1.</span> <span class="nav-text">RDD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark-streaming"><span class="nav-number">1.2.</span> <span class="nav-text">Spark streaming</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Graphx"><span class="nav-number">1.3.</span> <span class="nav-text">Graphx</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark-SQL"><span class="nav-number">1.4.</span> <span class="nav-text">Spark SQL</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SparkContext类和SparkConf类"><span class="nav-number">2.</span> <span class="nav-text">SparkContext类和SparkConf类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#广播变量（broadcast）和累加器（accumulator）"><span class="nav-number">2.1.</span> <span class="nav-text">广播变量（broadcast）和累加器（accumulator）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#parallelize和分区（partitions）"><span class="nav-number">2.2.</span> <span class="nav-text">parallelize和分区（partitions）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#textFile"><span class="nav-number">2.3.</span> <span class="nav-text">textFile</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#wholeTextFiles-path-minPartitions-None-sue-unicode-True"><span class="nav-number">2.4.</span> <span class="nav-text">wholeTextFiles(path,minPartitions = None,sue_unicode = True)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#addFile-path-recursive-False"><span class="nav-number">2.5.</span> <span class="nav-text">addFile(path,recursive=False)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#addPyFile-path"><span class="nav-number">2.6.</span> <span class="nav-text">addPyFile(path)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pickleFile"><span class="nav-number">2.7.</span> <span class="nav-text">pickleFile</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sequenceFile"><span class="nav-number">2.8.</span> <span class="nav-text">sequenceFile</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cancelAllJobs"><span class="nav-number">2.9.</span> <span class="nav-text">cancelAllJobs()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cancelJobGroup-groupId"><span class="nav-number">2.10.</span> <span class="nav-text">cancelJobGroup(groupId)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#setJobGroup"><span class="nav-number">2.11.</span> <span class="nav-text">setJobGroup</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#runJob-rdd-partitionFunc-partitions-None-allowLocal-False"><span class="nav-number">2.12.</span> <span class="nav-text">runJob(rdd,partitionFunc, partitions=None, allowLocal=False)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#emptyRDD"><span class="nav-number">2.13.</span> <span class="nav-text">emptyRDD()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#hadoopRdd"><span class="nav-number">2.14.</span> <span class="nav-text">hadoopRdd</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#getConf"><span class="nav-number">2.15.</span> <span class="nav-text">getConf()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#getLocalProperty-key"><span class="nav-number">2.16.</span> <span class="nav-text">getLocalProperty(key)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#startTime"><span class="nav-number">2.17.</span> <span class="nav-text">startTime</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#stop"><span class="nav-number">2.18.</span> <span class="nav-text">stop()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#range-start-end-None-step-1-numSlices-None"><span class="nav-number">2.19.</span> <span class="nav-text">range(start, end=None,step = 1, numSlices = None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#unions-rdds"><span class="nav-number">2.20.</span> <span class="nav-text">unions(rdds)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD的优先位置（preferredLocations）"><span class="nav-number">2.21.</span> <span class="nav-text">RDD的优先位置（preferredLocations）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#操作RDD"><span class="nav-number">3.</span> <span class="nav-text">操作RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#partitionBy-numPartitions-parttionFunc-…"><span class="nav-number">3.1.</span> <span class="nav-text">partitionBy(numPartitions, parttionFunc=…)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#repartition-numPartitions"><span class="nav-number">3.2.</span> <span class="nav-text">repartition(numPartitions)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#repaetitionAndSortWithPartitions-…"><span class="nav-number">3.3.</span> <span class="nav-text">repaetitionAndSortWithPartitions( …)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#coalesce-numPartitions-shuffle-False"><span class="nav-number">3.4.</span> <span class="nav-text">coalesce(numPartitions, shuffle = False)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#randomSplit-weights-seed-None"><span class="nav-number">3.5.</span> <span class="nav-text">randomSplit(weights, seed = None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#keyBy"><span class="nav-number">3.6.</span> <span class="nav-text">keyBy()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#keys"><span class="nav-number">3.7.</span> <span class="nav-text">keys()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#values"><span class="nav-number">3.8.</span> <span class="nav-text">values()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#collect"><span class="nav-number">3.9.</span> <span class="nav-text">collect()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#collectAsMap"><span class="nav-number">3.10.</span> <span class="nav-text">collectAsMap()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#persist-storageLevel-StorageLevel-False-True-False-False-1"><span class="nav-number">3.11.</span> <span class="nav-text">persist(storageLevel=StorageLevel(False, True, False, False, 1))</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cache"><span class="nav-number">3.12.</span> <span class="nav-text">cache()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#count"><span class="nav-number">3.13.</span> <span class="nav-text">count()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#countByKey"><span class="nav-number">3.14.</span> <span class="nav-text">countByKey</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#countByValue"><span class="nav-number">3.15.</span> <span class="nav-text">countByValue()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#intersection-other"><span class="nav-number">3.16.</span> <span class="nav-text">intersection(other)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#union-other"><span class="nav-number">3.17.</span> <span class="nav-text">union(other)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#subtract-other-numPartitions-None"><span class="nav-number">3.18.</span> <span class="nav-text">subtract(other, numPartitions=None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#subtractByKey-other-numPartitions-None"><span class="nav-number">3.19.</span> <span class="nav-text">subtractByKey(other, numPartitions=None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sum"><span class="nav-number">3.20.</span> <span class="nav-text">sum()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#histogram-buckets"><span class="nav-number">3.21.</span> <span class="nav-text">histogram(buckets)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mean"><span class="nav-number">3.22.</span> <span class="nav-text">mean()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#stdev"><span class="nav-number">3.23.</span> <span class="nav-text">stdev()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#variance"><span class="nav-number">3.24.</span> <span class="nav-text">variance()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#stats"><span class="nav-number">3.25.</span> <span class="nav-text">stats()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sample-withReplacement-fraction-seed-None"><span class="nav-number">3.26.</span> <span class="nav-text">sample(withReplacement, fraction, seed=None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sampleByKey-withPrplacement-fractions-seed-None"><span class="nav-number">3.27.</span> <span class="nav-text">sampleByKey(withPrplacement, fractions, seed = None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sampleStdev"><span class="nav-number">3.28.</span> <span class="nav-text">sampleStdev()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sampleVariance"><span class="nav-number">3.29.</span> <span class="nav-text">sampleVariance()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#zipWithIndex"><span class="nav-number">3.30.</span> <span class="nav-text">zipWithIndex()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#zipWithUniqueId"><span class="nav-number">3.31.</span> <span class="nav-text">zipWithUniqueId()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sortBy-keyfunc-ascending-True-numPartitions-None"><span class="nav-number">3.32.</span> <span class="nav-text">sortBy(keyfunc, ascending=True, numPartitions=None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sortByKey-ascending-True-numPartitions-None-keyfunc-…"><span class="nav-number">3.33.</span> <span class="nav-text">sortByKey(ascending=True, numPartitions=None, keyfunc=…)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#glom"><span class="nav-number">3.34.</span> <span class="nav-text">glom()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#foreach-f"><span class="nav-number">3.35.</span> <span class="nav-text">foreach(f)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#foreachPartition-f"><span class="nav-number">3.36.</span> <span class="nav-text">foreachPartition(f)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pipe-command-env-None-checkCode-False"><span class="nav-number">3.37.</span> <span class="nav-text">pipe(command, env=None, checkCode=False)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cartesian-other"><span class="nav-number">3.38.</span> <span class="nav-text">cartesian(other)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#aggregate-zeroValue-seqOp-combOp"><span class="nav-number">3.39.</span> <span class="nav-text">aggregate(zeroValue, seqOp, combOp)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#aggregateByKey"><span class="nav-number">3.40.</span> <span class="nav-text">aggregateByKey</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#groupBy"><span class="nav-number">3.41.</span> <span class="nav-text">groupBy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#groupByKey"><span class="nav-number">3.42.</span> <span class="nav-text">groupByKey</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cogroup-other-numPartitions-None"><span class="nav-number">3.43.</span> <span class="nav-text">cogroup(other, numPartitions=None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#groupWith-other-others"><span class="nav-number">3.44.</span> <span class="nav-text">groupWith(other, *others)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#combineByKey"><span class="nav-number">3.45.</span> <span class="nav-text">combineByKey</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#join-other-numPartitions-None"><span class="nav-number">3.46.</span> <span class="nav-text">join(other, numPartitions = None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#fullOuterJoin-other-numPartitions-None"><span class="nav-number">3.47.</span> <span class="nav-text">fullOuterJoin(other, numPartitions = None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#leftOuterJoin-other-numPartitions-None"><span class="nav-number">3.48.</span> <span class="nav-text">leftOuterJoin(other, numPartitions=None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#rightOuterJoin"><span class="nav-number">3.49.</span> <span class="nav-text">rightOuterJoin</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#fold-zeroValue-op"><span class="nav-number">3.50.</span> <span class="nav-text">fold(zeroValue, op)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#foldByKey"><span class="nav-number">3.51.</span> <span class="nav-text">foldByKey</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#map-f-preservesPartitioning-False-和flatmap"><span class="nav-number">3.52.</span> <span class="nav-text">map(f, preservesPartitioning = False)和flatmap</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#flatMapValues-f"><span class="nav-number">3.53.</span> <span class="nav-text">flatMapValues(f)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mapvalues-f"><span class="nav-number">3.54.</span> <span class="nav-text">mapvalues(f)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mapPartitions-f-preservesPartitioning-False"><span class="nav-number">3.55.</span> <span class="nav-text">mapPartitions(f, preservesPartitioning=False)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mapPartitionsWithIndex-f-preservesPartitioning-False"><span class="nav-number">3.56.</span> <span class="nav-text">mapPartitionsWithIndex(f, preservesPartitioning=False)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#filter-f"><span class="nav-number">3.57.</span> <span class="nav-text">filter(f)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#reduce-f"><span class="nav-number">3.58.</span> <span class="nav-text">reduce(f)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#reduceByKey-func-numPartitions-None-parititonFunc-…"><span class="nav-number">3.59.</span> <span class="nav-text">reduceByKey(func, numPartitions = None, parititonFunc = …)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#reduceByKeyLocally-func"><span class="nav-number">3.60.</span> <span class="nav-text">reduceByKeyLocally(func)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#treeReduce-f-depth-2"><span class="nav-number">3.61.</span> <span class="nav-text">treeReduce(f, depth=2)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lookup-key"><span class="nav-number">3.62.</span> <span class="nav-text">lookup(key)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#first"><span class="nav-number">3.63.</span> <span class="nav-text">first()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#top-num-key-None"><span class="nav-number">3.64.</span> <span class="nav-text">top(num, key = None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#max-key-None"><span class="nav-number">3.65.</span> <span class="nav-text">max(key = None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#min-key-None"><span class="nav-number">3.66.</span> <span class="nav-text">min(key = None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#take-num"><span class="nav-number">3.67.</span> <span class="nav-text">take(num)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#takeOrdered-num-key-None"><span class="nav-number">3.68.</span> <span class="nav-text">takeOrdered(num, key = None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#takeSample-withReplacement-num-seed-None"><span class="nav-number">3.69.</span> <span class="nav-text">takeSample(withReplacement, num, seed=None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#toLocalIterator"><span class="nav-number">3.70.</span> <span class="nav-text">toLocalIterator()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#id"><span class="nav-number">3.71.</span> <span class="nav-text">id()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#setName-name"><span class="nav-number">3.72.</span> <span class="nav-text">setName(name)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#getNumPartitions"><span class="nav-number">3.73.</span> <span class="nav-text">getNumPartitions()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#name"><span class="nav-number">3.74.</span> <span class="nav-text">name()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#isEmpty"><span class="nav-number">3.75.</span> <span class="nav-text">isEmpty()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#外部数据集"><span class="nav-number">3.76.</span> <span class="nav-text">外部数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#saveAsHadoopDataset-conf-keyConverter-none-valueConverter-None"><span class="nav-number">3.76.1.</span> <span class="nav-text">saveAsHadoopDataset(conf, keyConverter = none, valueConverter = None)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#saveAsNewAPIHadoopDataset-conf-keyConverter-None-valueConverter-None"><span class="nav-number">3.76.2.</span> <span class="nav-text">saveAsNewAPIHadoopDataset(conf, keyConverter=None, valueConverter=None)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#saveAsHadoopFile-…"><span class="nav-number">3.76.3.</span> <span class="nav-text">saveAsHadoopFile(…)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#saveAsNewAPIHadoopFile-…"><span class="nav-number">3.76.4.</span> <span class="nav-text">saveAsNewAPIHadoopFile(…)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#saveAsPickleFile-path-batchSize-10"><span class="nav-number">3.76.5.</span> <span class="nav-text">saveAsPickleFile(path, batchSize=10)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#saveAsSequenceFile-path-compressionCodecClass-None"><span class="nav-number">3.76.6.</span> <span class="nav-text">saveAsSequenceFile(path, compressionCodecClass=None)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#saveAsTextFile-path-compressionCodecClass-None"><span class="nav-number">3.76.7.</span> <span class="nav-text">saveAsTextFile(path, compressionCodecClass=None)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#外部传递函数给spark"><span class="nav-number">3.77.</span> <span class="nav-text">外部传递函数给spark</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#理解闭包"><span class="nav-number">3.78.</span> <span class="nav-text">理解闭包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#键-值对"><span class="nav-number">3.79.</span> <span class="nav-text">键-值对</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Shuffle操作"><span class="nav-number">3.80.</span> <span class="nav-text">Shuffle操作</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#性能影响"><span class="nav-number">3.80.1.</span> <span class="nav-text">性能影响</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD-Persistence（持久化）"><span class="nav-number">3.81.</span> <span class="nav-text">RDD Persistence（持久化）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#如何选择存储级别"><span class="nav-number">3.81.1.</span> <span class="nav-text">如何选择存储级别</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#删除数据"><span class="nav-number">3.81.2.</span> <span class="nav-text">删除数据</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署应用到集群中"><span class="nav-number">4.</span> <span class="nav-text">部署应用到集群中</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#单元测试"><span class="nav-number">5.</span> <span class="nav-text">单元测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#例程"><span class="nav-number">6.</span> <span class="nav-text">例程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pyspark-sql-module"><span class="nav-number">7.</span> <span class="nav-text">pyspark.sql module</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Dataset：Spark新的抽象层"><span class="nav-number">7.1.</span> <span class="nav-text">Dataset：Spark新的抽象层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark-Session：Spark-2-0入口"><span class="nav-number">7.2.</span> <span class="nav-text">Spark Session：Spark 2.0入口</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#class-pyspark-sql-SparkSession"><span class="nav-number">7.3.</span> <span class="nav-text">class pyspark.sql.SparkSession</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#class-pyspark-sql-HiveContext-sparkContext-jhiveContext-None"><span class="nav-number">7.3.1.</span> <span class="nav-text">class pyspark.sql.HiveContext(sparkContext, jhiveContext=None)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#读数据"><span class="nav-number">7.3.2.</span> <span class="nav-text">读数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#buider"><span class="nav-number">7.3.3.</span> <span class="nav-text">buider</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SparkSession-catalog"><span class="nav-number">7.3.4.</span> <span class="nav-text">SparkSession.catalog</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SparkSession-conf"><span class="nav-number">7.3.5.</span> <span class="nav-text">SparkSession.conf</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SparkSession-createDataFrame"><span class="nav-number">7.3.6.</span> <span class="nav-text">SparkSession.createDataFrame</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SparkSession-newSession"><span class="nav-number">7.3.7.</span> <span class="nav-text">SparkSession.newSession()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SparkSession-range-start-end-None-step-1-numPartitions-None"><span class="nav-number">7.3.8.</span> <span class="nav-text">SparkSession.range(start, end=None, step=1, numPartitions=None)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SparkSession-sparkContext"><span class="nav-number">7.3.9.</span> <span class="nav-text">SparkSession.sparkContext</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SparkSession-read"><span class="nav-number">7.3.10.</span> <span class="nav-text">SparkSession.read</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SparkSession-sql（sqlQuery）"><span class="nav-number">7.3.11.</span> <span class="nav-text">SparkSession.sql（sqlQuery）</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#SparkSession-stop"><span class="nav-number">7.3.11.1.</span> <span class="nav-text">SparkSession.stop()</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SparkSession-table-tableName"><span class="nav-number">7.3.12.</span> <span class="nav-text">SparkSession.table(tableName)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#class-pyspark-sql-SQLContext-sparkContext-sparkSession-None-jsqlContext-None"><span class="nav-number">7.4.</span> <span class="nav-text">class pyspark.sql.SQLContext(sparkContext, sparkSession=None, jsqlContext=None)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#class-pyspark-sql-HiveContext-sparkContext-jhiveContext-None-1"><span class="nav-number">7.5.</span> <span class="nav-text">class pyspark.sql.HiveContext(sparkContext, jhiveContext=None)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#refreshTable-tableName"><span class="nav-number">7.5.1.</span> <span class="nav-text">refreshTable(tableName)</span></a></li></ol></li></ol></li></ol>
    
    </div>
  </aside>
</section>
        
      </div>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2018 Zoe.Xiao&#39;s Blog All Rights Reserved.
        
            <span id="busuanzi_container_site_uv">
              本站访客数<span id="busuanzi_value_site_uv"></span>人次  
              本站总访问量<span id="busuanzi_value_site_pv"></span>次
            </span>
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hipaper" target="_blank">hipaper</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");

    wrapdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";


    <!-- headerblur min height -->
    
      var headerblur = document.getElementById("header-blur");
      headerblur.style.minHeight = window.getComputedStyle(document.getElementById("allheader"), null).height;
    
    
</script>
    
<div style="display: none;">
  <script src="https://s11.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
</div>

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/bootstrap.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
